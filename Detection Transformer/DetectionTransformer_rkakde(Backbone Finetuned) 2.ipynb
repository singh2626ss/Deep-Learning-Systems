{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Deep Learning Systems\n",
    "## Detection Transformer\n",
    "#### In this implementation I fine tune the model first with backbone frozen on a higher learning rate and then train the whole network including the backbone on a very small learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/Downloads/my_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-12 12:38:36.097608: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734007116.114464  158992 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734007116.119566  158992 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-12 12:38:36.138704: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import time  # For measuring time\n",
    "\n",
    "# Third-party imports\n",
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PIL (Python Imaging Library)\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# torchvision imports\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import DetrForObjectDetection, DetrConfig, DetrImageProcessor\n",
    "\n",
    "# pycocotools imports\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# timm (PyTorch Image Models) import\n",
    "import timm\n",
    "\n",
    "# Collections\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.66s)\n",
      "creating index...\n",
      "index created!\n",
      "Total images in the dataset: 29800\n",
      "Keys in the COCO annotations: ['info', 'licenses', 'categories', 'images', 'annotations']\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "data_folder = \"Self Driving Car/export/\"\n",
    "annotation_file = os.path.join(data_folder, \"_annotations.coco.json\")\n",
    "\n",
    "# Load COCO Annotations\n",
    "coco = COCO(annotation_file)\n",
    "\n",
    "# Print dataset statistics\n",
    "total_images = len(coco.getImgIds())\n",
    "print(f\"Total images in the dataset: {total_images}\")\n",
    "print(\"Keys in the COCO annotations:\", list(coco.dataset.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Inspect the annotation file to make sure it is suitable fro DETR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Keys in the COCO annotations: ['info', 'licenses', 'categories', 'images', 'annotations']\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Total images in the dataset: 29800\n",
      "Total annotations in the dataset: 194539\n",
      "Total categories in the dataset: 12\n",
      "Categories: ['obstacles', 'biker', 'car', 'pedestrian', 'trafficLight', 'trafficLight-Green', 'trafficLight-GreenLeft', 'trafficLight-Red', 'trafficLight-RedLeft', 'trafficLight-Yellow', 'trafficLight-YellowLeft', 'truck']\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Max classes per image: 7\n",
      "Min classes per image: 0\n",
      "Average classes per image: 1.88\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Sample image entry:\n",
      "{'id': 0, 'license': 1, 'file_name': '1478897026627294725_jpg.rf.6828a4e821cbab4c2c277d74df291f00.jpg', 'height': 512, 'width': 512, 'date_captured': '2021-06-09T12:24:25+00:00'}\n",
      "Sample annotation entry:\n",
      "{'id': 0, 'image_id': 0, 'category_id': 2, 'bbox': [140, 262, 21, 25.5], 'area': 535.5, 'segmentation': [], 'iscrowd': 0}\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print (\"--------------------------------------------------------\")\n",
    "# Check the structure of the JSON file\n",
    "print(\"Keys in the COCO annotations:\", list(coco.dataset.keys()))\n",
    "\n",
    "print (\"--------------------------------------------------------\")\n",
    "print (\"--------------------------------------------------------\")\n",
    "# Total number of images\n",
    "num_images = len(coco.getImgIds())\n",
    "print(f\"Total images in the dataset: {num_images}\")\n",
    "\n",
    "# Total number of annotations\n",
    "num_annotations = len(coco.getAnnIds())\n",
    "print(f\"Total annotations in the dataset: {num_annotations}\")\n",
    "\n",
    "# Total number of categories\n",
    "num_categories = len(coco.getCatIds())\n",
    "print(f\"Total categories in the dataset: {num_categories}\")\n",
    "\n",
    "# List all categories\n",
    "categories = coco.loadCats(coco.getCatIds())\n",
    "print(\"Categories:\", [cat['name'] for cat in categories])\n",
    "\n",
    "print (\"--------------------------------------------------------\")\n",
    "print (\"--------------------------------------------------------\")\n",
    "\n",
    "# Map image IDs to annotations\n",
    "img_to_anns = defaultdict(list)\n",
    "for ann in coco.loadAnns(coco.getAnnIds()):\n",
    "    img_to_anns[ann['image_id']].append(ann['category_id'])\n",
    "\n",
    "# Calculate the number of unique classes per image\n",
    "classes_per_image = [len(set(img_to_anns[img_id])) for img_id in coco.getImgIds()]\n",
    "\n",
    "# Statistics\n",
    "max_classes = max(classes_per_image)\n",
    "min_classes = min(classes_per_image)\n",
    "avg_classes = np.mean(classes_per_image)\n",
    "\n",
    "print(f\"Max classes per image: {max_classes}\")\n",
    "print(f\"Min classes per image: {min_classes}\")\n",
    "print(f\"Average classes per image: {avg_classes:.2f}\")\n",
    "\n",
    "print (\"--------------------------------------------------------\")\n",
    "print (\"--------------------------------------------------------\")\n",
    "# Example image entry\n",
    "sample_image_id = coco.getImgIds()[0]\n",
    "sample_image = coco.loadImgs(sample_image_id)[0]\n",
    "print(\"Sample image entry:\")\n",
    "print(sample_image)\n",
    "\n",
    "# Example annotation\n",
    "sample_ann_id = coco.getAnnIds(imgIds=sample_image_id)[0]\n",
    "sample_annotation = coco.loadAnns(sample_ann_id)[0]\n",
    "print(\"Sample annotation entry:\")\n",
    "print(sample_annotation)\n",
    "\n",
    "\n",
    "print (\"--------------------------------------------------------\")\n",
    "print (\"--------------------------------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output confirms that the dataset is in a **COCO-compatible format** and is suitable for use with **DETR**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Key Details About the Dataset**\n",
    "- **Total Images**: 29,800\n",
    "- **Total Annotations**: 194,539\n",
    "- **Total Categories**: 12\n",
    "- **Categories**:\n",
    "  ```plaintext\n",
    "  ['obstacles', 'biker', 'car', 'pedestrian', 'trafficLight', \n",
    "   'trafficLight-Green', 'trafficLight-GreenLeft', 'trafficLight-Red',\n",
    "   'trafficLight-RedLeft', 'trafficLight-Yellow', 'trafficLight-YellowLeft', 'truck']\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Classes Per Image**\n",
    "- **Max Classes per Image**: 7\n",
    "  - Some images have up to 7 unique object categories.\n",
    "- **Min Classes per Image**: 0\n",
    "  - Some images have no objects annotated (annotations might be missing or labeled as empty).\n",
    "- **Average Classes per Image**: 1.88\n",
    "  - On average, each image contains nearly 2 unique categories.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Annotation Format (Sample Entry)**\n",
    "- **Image Entry**:\n",
    "  ```python\n",
    "  {'id': 0, 'license': 1, 'file_name': '1478897026627294725_jpg.rf.6828a4e821cbab4c2c277d74df291f00.jpg', \n",
    "   'height': 512, 'width': 512, 'date_captured': '2021-06-09T12:24:25+00:00'}\n",
    "  ```\n",
    "  - Contains `id`, `file_name`, and dimensions (`height` and `width`), which are essential for DETR training.\n",
    "\n",
    "- **Annotation Entry**:\n",
    "  ```python\n",
    "  {'id': 0, 'image_id': 0, 'category_id': 2, 'bbox': [140, 262, 21, 25.5], \n",
    "   'area': 535.5, 'segmentation': [], 'iscrowd': 0}\n",
    "  ```\n",
    "  - **`image_id`** links the annotation to the corresponding image.\n",
    "  - **`category_id`** identifies the object category.\n",
    "  - **`bbox`** is in the correct COCO format `[x_min, y_min, width, height]`.\n",
    "  - **`area`** is calculated (but not mandatory for DETR).\n",
    "  - **`iscrowd`** is set to `0`, which is the expected value for single objects (not groups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 25330, Validation images: 2235, Testing images: 2235\n"
     ]
    }
   ],
   "source": [
    "# Dataset splits\n",
    "num_train = 25330\n",
    "num_val = 2235\n",
    "num_test = 2235\n",
    "\n",
    "# Shuffle and split the dataset\n",
    "all_image_ids = coco.getImgIds()\n",
    "random.seed(42)  # Ensure reproducibility\n",
    "random.shuffle(all_image_ids)\n",
    "\n",
    "train_ids = all_image_ids[:num_train]\n",
    "val_ids = all_image_ids[num_train:num_train + num_val]\n",
    "test_ids = all_image_ids[num_train + num_val:num_train + num_val + num_test]\n",
    "\n",
    "print(f\"Training images: {len(train_ids)}, Validation images: {len(val_ids)}, Testing images: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom COCO dataset\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, image_dir, coco, processor, image_ids):\n",
    "        self.image_dir = image_dir\n",
    "        self.coco = coco\n",
    "        self.processor = processor\n",
    "        self.image_ids = image_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.image_dir, image_info[\"file_name\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Get annotations\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        annotations = []\n",
    "\n",
    "        for ann in anns:\n",
    "            bbox = ann[\"bbox\"]  # [x_min, y_min, width, height]\n",
    "            area = bbox[2] * bbox[3]  # width * height\n",
    "            annotations.append({\n",
    "                \"bbox\": bbox,\n",
    "                \"category_id\": ann[\"category_id\"],  # Preserve original category IDs\n",
    "                \"area\": area,\n",
    "                \"iscrowd\": ann.get(\"iscrowd\", 0),\n",
    "            })\n",
    "\n",
    "        # Prepare the target dictionary\n",
    "        target = {\"image_id\": image_id, \"annotations\": annotations}\n",
    "\n",
    "        # Process the image and annotations\n",
    "        encoding = self.processor(images=image, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze(0)\n",
    "        labels = encoding[\"labels\"][0]\n",
    "        return pixel_values, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets prepared: 25330 training, 2235 validation, 2235 testing.\n"
     ]
    }
   ],
   "source": [
    "# Path to images\n",
    "image_dir = data_folder\n",
    "\n",
    "# Load pre-trained processor\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "# Create datasets for each split\n",
    "train_dataset = COCODataset(image_dir, coco, processor, train_ids)\n",
    "val_dataset = COCODataset(image_dir, coco, processor, val_ids)\n",
    "test_dataset = COCODataset(image_dir, coco, processor, test_ids)\n",
    "\n",
    "print(f\"Datasets prepared: {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders ready.\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "print(\"DataLoaders ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values shape: torch.Size([3, 800, 800])\n",
      "Target example: {'size': tensor([800, 800]), 'image_id': tensor([24418]), 'class_labels': tensor([2, 2, 2, 2, 2, 2, 2]), 'boxes': tensor([[0.0996, 0.5605, 0.1992, 0.2148],\n",
      "        [0.2197, 0.5493, 0.0645, 0.1338],\n",
      "        [0.2827, 0.5352, 0.0850, 0.1133],\n",
      "        [0.3262, 0.5288, 0.0469, 0.0850],\n",
      "        [0.3545, 0.5234, 0.0332, 0.0820],\n",
      "        [0.6870, 0.5322, 0.0967, 0.1152],\n",
      "        [0.8516, 0.5488, 0.2539, 0.2383]]), 'area': tensor([27392.5781,  5518.7988,  6159.6680,  2548.8281,  1743.1641,  7130.1270,\n",
      "        38720.7031]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0]), 'orig_size': tensor([512, 512])}\n"
     ]
    }
   ],
   "source": [
    "# Test a batch\n",
    "for pixel_values, targets in train_loader:\n",
    "    print(f\"Pixel values shape: {pixel_values[0].shape}\")\n",
    "    print(f\"Target example: {targets[0]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup and Initial Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Modify the Classification Head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "Number of classes: 13\n"
     ]
    }
   ],
   "source": [
    "# Initialize DETR with a custom configuration\n",
    "num_classes = 12  # 12 object classes + 1 background (created automatically)\n",
    "config = DetrConfig(num_labels=num_classes, ignore_mismatched_sizes=True, backbone=\"resnet50\")\n",
    "model = DetrForObjectDetection(config)\n",
    "\n",
    "# Create the Backbone Using timm\n",
    "# Create the pre-trained timm ResNet-50 backbone\n",
    "timm_backbone = timm.create_model(\"resnet50\", pretrained=True, features_only=True, out_indices=(1, 2, 3, 4))\n",
    "\n",
    "# Transfer Weights and Buffers\n",
    "for name_backbone, parameter_backbone in timm_backbone.named_parameters():\n",
    "    for name, parameter in model.model.backbone.conv_encoder.model.named_parameters():\n",
    "        if name_backbone == name:\n",
    "            parameter.data.copy_(parameter_backbone.data)\n",
    "\n",
    "# Transfer buffers (e.g., running mean and variance in BatchNorm)\n",
    "for name_backbone, buffer_backbone in timm_backbone.named_buffers():\n",
    "    for name, buffer in model.model.backbone.conv_encoder.model.named_buffers():\n",
    "        if name_backbone == name:\n",
    "            buffer.data.copy_(buffer_backbone.data)\n",
    "\n",
    "print((model.model.backbone.conv_encoder.model.conv1.weight == timm_backbone.conv1.weight).all())\n",
    "print(\"Number of classes:\", model.class_labels_classifier.out_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 1: Fine-Tuning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is relatively small, freezing the ResNet backbone will prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Freeze the backbone\n",
    "for param in model.model.backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the architechture of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetrForObjectDetection(\n",
      "  (model): DetrModel(\n",
      "    (backbone): DetrConvModel(\n",
      "      (conv_encoder): DetrConvEncoder(\n",
      "        (model): FeatureListNet(\n",
      "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "          (bn1): DetrFrozenBatchNorm2d()\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "          (layer1): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): DetrFrozenBatchNorm2d()\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer2): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): DetrFrozenBatchNorm2d()\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (3): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer3): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): DetrFrozenBatchNorm2d()\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (3): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (4): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (5): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer4): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): DetrFrozenBatchNorm2d()\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (position_embedding): DetrSinePositionEmbedding()\n",
      "    )\n",
      "    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (query_position_embeddings): Embedding(100, 256)\n",
      "    (encoder): DetrEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x DetrEncoderLayer(\n",
      "          (self_attn): DetrAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): ReLU()\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): DetrDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x DetrDecoderLayer(\n",
      "          (self_attn): DetrAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): DetrAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (class_labels_classifier): Linear(in_features=256, out_features=13, bias=True)\n",
      "  (bbox_predictor): DetrMLPPredictionHead(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up Optimizer and Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DetrForObjectDetection(\n",
       "  (model): DetrModel(\n",
       "    (backbone): DetrConvModel(\n",
       "      (conv_encoder): DetrConvEncoder(\n",
       "        (model): FeatureListNet(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): DetrFrozenBatchNorm2d()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (layer1): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer2): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer3): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (4): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (5): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer4): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (position_embedding): DetrSinePositionEmbedding()\n",
       "    )\n",
       "    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (query_position_embeddings): Embedding(100, 256)\n",
       "    (encoder): DetrEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetrEncoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): DetrDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetrDecoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (class_labels_classifier): Linear(in_features=256, out_features=13, bias=True)\n",
       "  (bbox_predictor): DetrMLPPredictionHead(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizer for Stage 1\n",
    "optimizer_stage1 = AdamW(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "\n",
    "# Learning rate scheduler for Stage 1\n",
    "scheduler_stage1 = StepLR(optimizer_stage1, step_size=10, gamma=0.1)\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Stage 1: Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stage 1: Training with frozen backbone...\n",
      "Epoch 1/10, Loss: 3.5675, Time: 1259.18 seconds\n",
      "Epoch 2/10, Loss: 3.3067, Time: 1291.24 seconds\n",
      "Epoch 3/10, Loss: 3.2784, Time: 1299.65 seconds\n",
      "Epoch 4/10, Loss: 3.2585, Time: 1363.01 seconds\n",
      "Epoch 5/10, Loss: 3.2090, Time: 1301.09 seconds\n",
      "Epoch 6/10, Loss: 3.2017, Time: 1305.78 seconds\n",
      "Epoch 7/10, Loss: 3.2035, Time: 1295.48 seconds\n",
      "Epoch 8/10, Loss: 3.2247, Time: 1290.83 seconds\n",
      "Epoch 9/10, Loss: 3.1858, Time: 1257.32 seconds\n",
      "Epoch 10/10, Loss: 3.1841, Time: 1294.08 seconds\n"
     ]
    }
   ],
   "source": [
    "# Stage 1: Train the model with a frozen backbone\n",
    "num_epochs_stage1 = 10  # Initial training epochs with frozen backbone\n",
    "\n",
    "print(\"Starting Stage 1: Training with frozen backbone...\")\n",
    "total_training_start_time = time.time()\n",
    "for epoch in range(num_epochs_stage1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for step, (pixel_values, targets) in enumerate(train_loader):\n",
    "        pixel_values = torch.stack(pixel_values).to(device)\n",
    "\n",
    "        # Filter valid targets\n",
    "        valid_targets = []\n",
    "        for t in targets:\n",
    "            if \"class_labels\" in t and len(t[\"class_labels\"]) > 0:\n",
    "                valid_targets.append({\n",
    "                    \"class_labels\": t[\"class_labels\"].to(device),\n",
    "                    \"boxes\": t[\"boxes\"].to(device)\n",
    "                })\n",
    "\n",
    "        while len(valid_targets) < len(pixel_values):\n",
    "            valid_targets.append({\n",
    "                \"class_labels\": torch.empty((0,), dtype=torch.int64).to(device),\n",
    "                \"boxes\": torch.empty((0, 4), dtype=torch.float32).to(device)\n",
    "            })\n",
    "\n",
    "        if all(len(t[\"class_labels\"]) == 0 for t in valid_targets):\n",
    "            #print(f\"Skipping batch {step}: No valid targets.\")\n",
    "            continue\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=pixel_values, labels=valid_targets)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer_stage1.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_stage1.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler_stage1.step()\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs_stage1}, Loss: {total_loss / len(train_loader):.4f}, Time: {epoch_end_time - epoch_start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 2: Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stage 2: Training with unfrozen backbone...\n"
     ]
    }
   ],
   "source": [
    "# Stage 2: Unfreeze the backbone and train the entire model\n",
    "print(\"Starting Stage 2: Training with unfrozen backbone...\")\n",
    "\n",
    "for param in model.model.backbone.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Optimizer for Stage 2\n",
    "optimizer_stage2 = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler for Stage 2\n",
    "scheduler_stage2 = StepLR(optimizer_stage2, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Stage 2: Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 3.1169, Time: 1459.90 seconds\n",
      "Epoch 2/5, Loss: 3.1069, Time: 1409.05 seconds\n",
      "Epoch 3/5, Loss: 3.1050, Time: 1454.91 seconds\n",
      "Epoch 4/5, Loss: 3.0984, Time: 1459.00 seconds\n",
      "Epoch 5/5, Loss: 3.0973, Time: 1433.01 seconds\n"
     ]
    }
   ],
   "source": [
    "num_epochs_stage2 = 5  # Remaining epochs for fine-tuning\n",
    "\n",
    "for epoch in range(num_epochs_stage2):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for step, (pixel_values, targets) in enumerate(train_loader):\n",
    "        pixel_values = torch.stack(pixel_values).to(device)\n",
    "\n",
    "        valid_targets = []\n",
    "        for t in targets:\n",
    "            if \"class_labels\" in t and len(t[\"class_labels\"]) > 0:\n",
    "                valid_targets.append({\n",
    "                    \"class_labels\": t[\"class_labels\"].to(device),\n",
    "                    \"boxes\": t[\"boxes\"].to(device)\n",
    "                })\n",
    "\n",
    "        while len(valid_targets) < len(pixel_values):\n",
    "            valid_targets.append({\n",
    "                \"class_labels\": torch.empty((0,), dtype=torch.int64).to(device),\n",
    "                \"boxes\": torch.empty((0, 4), dtype=torch.float32).to(device)\n",
    "            })\n",
    "\n",
    "        if all(len(t[\"class_labels\"]) == 0 for t in valid_targets):\n",
    "            #print(f\"Skipping batch {step}: No valid targets.\")\n",
    "            continue\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, labels=valid_targets)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer_stage2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_stage2.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler_stage2.step()\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs_stage2}, Loss: {total_loss / len(train_loader):.4f}, Time: {epoch_end_time - epoch_start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time: 20173.56 seconds\n"
     ]
    }
   ],
   "source": [
    "total_training_end_time = time.time()\n",
    "print(f\"Total Training Time: {total_training_end_time - total_training_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DETR_BackboneTrained/detr-processor/preprocessor_config.json']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"DETR_BackboneTrained/detr-finetuned\")\n",
    "processor.save_pretrained(\"DETR_BackboneTrained/detr-processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 3: Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stage 3: Additional fine-tuning...\n",
      "Skipping batch 1057: No valid targets.\n",
      "Skipping batch 3036: No valid targets.\n",
      "Skipping batch 3213: No valid targets.\n",
      "Epoch 1/10, Loss: 3.0967, Time: 1460.79 seconds\n",
      "Skipping batch 4472: No valid targets.\n",
      "Epoch 2/10, Loss: 3.0974, Time: 1430.20 seconds\n",
      "Skipping batch 5051: No valid targets.\n",
      "Skipping batch 5250: No valid targets.\n",
      "Epoch 3/10, Loss: 3.0951, Time: 1430.22 seconds\n",
      "Skipping batch 2406: No valid targets.\n",
      "Skipping batch 5561: No valid targets.\n",
      "Skipping batch 6085: No valid targets.\n",
      "Epoch 4/10, Loss: 3.0914, Time: 1434.37 seconds\n",
      "Epoch 5/10, Loss: 3.0917, Time: 1437.66 seconds\n",
      "Epoch 6/10, Loss: 3.0952, Time: 1420.01 seconds\n",
      "Skipping batch 1972: No valid targets.\n",
      "Epoch 7/10, Loss: 3.0934, Time: 1413.08 seconds\n",
      "Skipping batch 6090: No valid targets.\n",
      "Epoch 8/10, Loss: 3.0899, Time: 1436.00 seconds\n",
      "Skipping batch 5718: No valid targets.\n",
      "Epoch 9/10, Loss: 3.0849, Time: 1447.30 seconds\n",
      "Skipping batch 2590: No valid targets.\n",
      "Epoch 10/10, Loss: 3.0904, Time: 1440.77 seconds\n",
      "Total Training Time: 79243.44 seconds\n"
     ]
    }
   ],
   "source": [
    "# Stage 3: Additional fine-tuning for 10 epochs\n",
    "print(\"Starting Stage 3: Additional fine-tuning...\")\n",
    "num_epochs_stage3 = 10  # Additional fine-tuning epochs\n",
    "\n",
    "for epoch in range(num_epochs_stage3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for step, (pixel_values, targets) in enumerate(train_loader):\n",
    "        pixel_values = torch.stack(pixel_values).to(device)\n",
    "\n",
    "        valid_targets = []\n",
    "        for t in targets:\n",
    "            if \"class_labels\" in t and len(t[\"class_labels\"]) > 0:\n",
    "                valid_targets.append({\n",
    "                    \"class_labels\": t[\"class_labels\"].to(device),\n",
    "                    \"boxes\": t[\"boxes\"].to(device)\n",
    "                })\n",
    "\n",
    "        while len(valid_targets) < len(pixel_values):\n",
    "            valid_targets.append({\n",
    "                \"class_labels\": torch.empty((0,), dtype=torch.int64).to(device),\n",
    "                \"boxes\": torch.empty((0, 4), dtype=torch.float32).to(device)\n",
    "            })\n",
    "\n",
    "        if all(len(t[\"class_labels\"]) == 0 for t in valid_targets):\n",
    "            print(f\"Skipping batch {step}: No valid targets.\")\n",
    "            continue\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, labels=valid_targets)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer_stage2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_stage2.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler_stage2.step()\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs_stage3}, Loss: {total_loss / len(train_loader):.4f}, Time: {epoch_end_time - epoch_start_time:.2f} seconds\")\n",
    "\n",
    "total_training_end_time = time.time()\n",
    "print(f\"Total Training Time: {total_training_end_time - total_training_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DETR_BackboneTrained_30epochs/detr-processor/preprocessor_config.json']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model after Stage 3\n",
    "model.save_pretrained(\"DETR_BackboneTrained_30epochs\")\n",
    "processor.save_pretrained(\"DETR_BackboneTrained_30epochs/detr-processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 4: Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stage 4: Additional fine-tuning...\n",
      "Epoch 1/10, Loss: 3.0896, Time: 1428.01 seconds\n",
      "Epoch 2/10, Loss: 3.0886, Time: 1438.90 seconds\n",
      "Epoch 3/10, Loss: 3.0916, Time: 1438.33 seconds\n",
      "Epoch 4/10, Loss: 3.0906, Time: 1527.62 seconds\n",
      "Epoch 5/10, Loss: 3.0868, Time: 1526.95 seconds\n",
      "Epoch 6/10, Loss: 3.0909, Time: 1503.87 seconds\n",
      "Epoch 7/10, Loss: 3.0930, Time: 1453.90 seconds\n",
      "Epoch 8/10, Loss: 3.0886, Time: 1423.42 seconds\n",
      "Epoch 9/10, Loss: 3.0916, Time: 1496.36 seconds\n",
      "Epoch 10/10, Loss: 3.0915, Time: 1488.25 seconds\n"
     ]
    }
   ],
   "source": [
    "# Stage 4: Additional fine-tuning for 10 more epochs\n",
    "print(\"Starting Stage 4: Additional fine-tuning...\")\n",
    "num_epochs_stage4 = 10  # Additional fine-tuning epochs\n",
    "\n",
    "for epoch in range(num_epochs_stage4):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for step, (pixel_values, targets) in enumerate(train_loader):\n",
    "        pixel_values = torch.stack(pixel_values).to(device)\n",
    "\n",
    "        valid_targets = []\n",
    "        for t in targets:\n",
    "            if \"class_labels\" in t and len(t[\"class_labels\"]) > 0:\n",
    "                valid_targets.append({\n",
    "                    \"class_labels\": t[\"class_labels\"].to(device),\n",
    "                    \"boxes\": t[\"boxes\"].to(device)\n",
    "                })\n",
    "\n",
    "        while len(valid_targets) < len(pixel_values):\n",
    "            valid_targets.append({\n",
    "                \"class_labels\": torch.empty((0,), dtype=torch.int64).to(device),\n",
    "                \"boxes\": torch.empty((0, 4), dtype=torch.float32).to(device)\n",
    "            })\n",
    "\n",
    "        if all(len(t[\"class_labels\"]) == 0 for t in valid_targets):\n",
    "            #print(f\"Skipping batch {step}: No valid targets.\")\n",
    "            continue\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, labels=valid_targets)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer_stage2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_stage2.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler_stage2.step()\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs_stage4}, Loss: {total_loss / len(train_loader):.4f}, Time: {epoch_end_time - epoch_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DETR_BackboneTrained_40epochs/detr-processor-stage4/preprocessor_config.json']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model after Stage 4\n",
    "model.save_pretrained(\"DETR_BackboneTrained_40epochs\")\n",
    "processor.save_pretrained(\"DETR_BackboneTrained_40epochs/detr-processor-stage4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 5: Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stage 5: Final fine-tuning stage...\n",
      "Epoch 1/10, Loss: 3.0906, Time: 1438.83 seconds\n",
      "Epoch 2/10, Loss: 3.0874, Time: 1450.17 seconds\n",
      "Epoch 3/10, Loss: 3.0899, Time: 1429.25 seconds\n",
      "Epoch 4/10, Loss: 3.0862, Time: 1431.89 seconds\n",
      "Epoch 5/10, Loss: 3.0882, Time: 1439.48 seconds\n",
      "Epoch 6/10, Loss: 3.0882, Time: 1443.86 seconds\n",
      "Epoch 7/10, Loss: 3.0939, Time: 1433.91 seconds\n",
      "Epoch 8/10, Loss: 3.0893, Time: 1428.52 seconds\n",
      "Epoch 9/10, Loss: 3.0928, Time: 1473.26 seconds\n",
      "Epoch 10/10, Loss: 3.0915, Time: 1435.82 seconds\n"
     ]
    }
   ],
   "source": [
    "# Stage 5: Additional fine-tuning for 10 more epochs\n",
    "print(\"Starting Stage 5: Final fine-tuning stage...\")\n",
    "num_epochs_stage5 = 10  # Additional fine-tuning epochs\n",
    "\n",
    "for epoch in range(num_epochs_stage5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for step, (pixel_values, targets) in enumerate(train_loader):\n",
    "        pixel_values = torch.stack(pixel_values).to(device)\n",
    "\n",
    "        valid_targets = []\n",
    "        for t in targets:\n",
    "            if \"class_labels\" in t and len(t[\"class_labels\"]) > 0:\n",
    "                valid_targets.append({\n",
    "                    \"class_labels\": t[\"class_labels\"].to(device),\n",
    "                    \"boxes\": t[\"boxes\"].to(device)\n",
    "                })\n",
    "\n",
    "        while len(valid_targets) < len(pixel_values):\n",
    "            valid_targets.append({\n",
    "                \"class_labels\": torch.empty((0,), dtype=torch.int64).to(device),\n",
    "                \"boxes\": torch.empty((0, 4), dtype=torch.float32).to(device)\n",
    "            })\n",
    "\n",
    "        if all(len(t[\"class_labels\"]) == 0 for t in valid_targets):\n",
    "            #print(f\"Skipping batch {step}: No valid targets.\")\n",
    "            continue\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, labels=valid_targets)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer_stage2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_stage2.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler_stage2.step()\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs_stage5}, Loss: {total_loss / len(train_loader):.4f}, Time: {epoch_end_time - epoch_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time Across All Stages: 108374.85 seconds\n"
     ]
    }
   ],
   "source": [
    "# Final training duration\n",
    "final_training_end_time = time.time()\n",
    "print(f\"Total Training Time Across All Stages: {final_training_end_time - total_training_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DETR_BackboneTrained_50epochs/detr-processor-stage5/preprocessor_config.json']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model after Stage 5\n",
    "model.save_pretrained(\"DETR_BackboneTrained_50epochs\")\n",
    "processor.save_pretrained(\"DETR_BackboneTrained_50epochs/detr-processor-stage5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 6 Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stage 6: Training the entire network for 30 epochs...\n"
     ]
    }
   ],
   "source": [
    "# Stage 6: Train the entire network (including the backbone) for 30 epochs\n",
    "print(\"Starting Stage 6: Training the entire network for 30 epochs...\")\n",
    "\n",
    "# Unfreeze the backbone\n",
    "for param in model.model.backbone.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Define a new optimizer for Stage 6\n",
    "optimizer_stage6 = AdamW(model.parameters(), lr=5e-3, weight_decay=1e-3)  \n",
    "scheduler_stage6 = StepLR(optimizer_stage6, step_size=10, gamma=0.1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 3.6027, Time: 1414.52 seconds\n",
      "Epoch 2/30, Loss: 3.5686, Time: 1452.51 seconds\n",
      "Epoch 3/30, Loss: 3.6112, Time: 1418.59 seconds\n",
      "Epoch 4/30, Loss: 3.6268, Time: 1424.36 seconds\n",
      "Epoch 5/30, Loss: 3.5275, Time: 1436.30 seconds\n",
      "Epoch 6/30, Loss: 3.5985, Time: 1488.77 seconds\n",
      "Epoch 7/30, Loss: 3.5090, Time: 1437.03 seconds\n",
      "Epoch 8/30, Loss: 3.5450, Time: 1469.96 seconds\n",
      "Epoch 9/30, Loss: 3.5824, Time: 1477.51 seconds\n",
      "Epoch 10/30, Loss: 3.5784, Time: 1439.48 seconds\n",
      "Epoch 11/30, Loss: 3.2199, Time: 1456.53 seconds\n",
      "Epoch 12/30, Loss: 3.1701, Time: 1465.53 seconds\n",
      "Epoch 13/30, Loss: 3.1668, Time: 1481.28 seconds\n",
      "Epoch 14/30, Loss: 3.1625, Time: 1457.98 seconds\n",
      "Epoch 15/30, Loss: 3.1657, Time: 1431.68 seconds\n",
      "Epoch 16/30, Loss: 3.1648, Time: 1468.02 seconds\n",
      "Epoch 17/30, Loss: 3.1702, Time: 1448.78 seconds\n",
      "Epoch 18/30, Loss: 3.1626, Time: 1459.74 seconds\n",
      "Epoch 19/30, Loss: 3.1610, Time: 1500.99 seconds\n",
      "Epoch 20/30, Loss: 3.1572, Time: 1447.51 seconds\n",
      "Epoch 21/30, Loss: 3.1346, Time: 1454.04 seconds\n",
      "Epoch 22/30, Loss: 3.1226, Time: 1477.80 seconds\n",
      "Epoch 23/30, Loss: 3.1173, Time: 1433.16 seconds\n",
      "Epoch 24/30, Loss: 3.1150, Time: 1453.68 seconds\n",
      "Epoch 25/30, Loss: 3.1188, Time: 1433.39 seconds\n",
      "Epoch 26/30, Loss: 3.1152, Time: 1425.03 seconds\n",
      "Epoch 27/30, Loss: 3.1123, Time: 1451.59 seconds\n",
      "Epoch 28/30, Loss: 3.1149, Time: 1446.67 seconds\n",
      "Epoch 29/30, Loss: 3.1163, Time: 1446.97 seconds\n",
      "Epoch 30/30, Loss: 3.1114, Time: 1460.29 seconds\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs for Stage 6\n",
    "num_epochs_stage6 = 30\n",
    "\n",
    "# Start training for Stage 6\n",
    "for epoch in range(num_epochs_stage6):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for step, (pixel_values, targets) in enumerate(train_loader):\n",
    "        # Move pixel values to the device\n",
    "        pixel_values = torch.stack(pixel_values).to(device)\n",
    "\n",
    "        # Filter valid targets\n",
    "        valid_targets = []\n",
    "        for t in targets:\n",
    "            if \"class_labels\" in t and len(t[\"class_labels\"]) > 0:\n",
    "                valid_targets.append({\n",
    "                    \"class_labels\": t[\"class_labels\"].to(device),\n",
    "                    \"boxes\": t[\"boxes\"].to(device)\n",
    "                })\n",
    "\n",
    "        # Add properly formatted dummy targets if necessary\n",
    "        while len(valid_targets) < len(pixel_values):\n",
    "            valid_targets.append({\n",
    "                \"class_labels\": torch.empty((0,), dtype=torch.int64).to(device),\n",
    "                \"boxes\": torch.empty((0, 4), dtype=torch.float32).to(device)\n",
    "            })\n",
    "\n",
    "        # Skip the batch if no valid targets exist\n",
    "        if all(len(t[\"class_labels\"]) == 0 for t in valid_targets):\n",
    "            #print(f\"Skipping batch {step}: No valid targets.\")\n",
    "            continue\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=pixel_values, labels=valid_targets)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer_stage6.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_stage6.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler_stage6.step()\n",
    "\n",
    "    # Record epoch duration\n",
    "    epoch_end_time = time.time()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs_stage6}, Loss: {total_loss / len(train_loader):.4f}, Time: {epoch_end_time - epoch_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 6 Training Time: 222919.86 seconds\n"
     ]
    }
   ],
   "source": [
    "# Save the model and processor after Stage 6\n",
    "model.save_pretrained(\"DETR_FullNetworkTrained_80epochs/detr-finetuned\")\n",
    "processor.save_pretrained(\"DETR/detr-processor-stage6\")\n",
    "\n",
    "# Final training time for Stage 6\n",
    "stage6_end_time = time.time()\n",
    "print(f\"Stage 6 Training Time: {stage6_end_time - total_training_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainability Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grad-CAM for DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6123423,
     "sourceId": 9956307,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
