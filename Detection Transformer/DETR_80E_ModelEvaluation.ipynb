{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETR Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/Downloads/my_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-15 13:15:41.707047: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734268541.725977  382983 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734268541.732386  382983 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 13:15:41.754951: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import time  # For measuring time\n",
    "\n",
    "# Third-party imports\n",
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PIL (Python Imaging Library)\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# torchvision imports\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import DetrForObjectDetection, DetrConfig, DetrImageProcessor\n",
    "\n",
    "# pycocotools imports\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# timm (PyTorch Image Models) import\n",
    "import timm\n",
    "\n",
    "# Collections\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.80s)\n",
      "creating index...\n",
      "index created!\n",
      "Total images in the dataset: 29800\n",
      "Keys in the COCO annotations: ['info', 'licenses', 'categories', 'images', 'annotations']\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "data_folder = \"Self Driving Car/export/\"\n",
    "annotation_file = os.path.join(data_folder, \"_annotations.coco.json\")\n",
    "\n",
    "# Load COCO Annotations\n",
    "coco = COCO(annotation_file)\n",
    "\n",
    "# Print dataset statistics\n",
    "total_images = len(coco.getImgIds())\n",
    "print(f\"Total images in the dataset: {total_images}\")\n",
    "print(\"Keys in the COCO annotations:\", list(coco.dataset.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Inspect the annotation file to make sure it is suitable fro DETR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Keys in the COCO annotations: ['info', 'licenses', 'categories', 'images', 'annotations']\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Total images in the dataset: 29800\n",
      "Total annotations in the dataset: 194539\n",
      "Total categories in the dataset: 12\n",
      "Categories: ['obstacles', 'biker', 'car', 'pedestrian', 'trafficLight', 'trafficLight-Green', 'trafficLight-GreenLeft', 'trafficLight-Red', 'trafficLight-RedLeft', 'trafficLight-Yellow', 'trafficLight-YellowLeft', 'truck']\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Max classes per image: 7\n",
      "Min classes per image: 0\n",
      "Average classes per image: 1.88\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Sample image entry:\n",
      "{'id': 0, 'license': 1, 'file_name': '1478897026627294725_jpg.rf.6828a4e821cbab4c2c277d74df291f00.jpg', 'height': 512, 'width': 512, 'date_captured': '2021-06-09T12:24:25+00:00'}\n",
      "Sample annotation entry:\n",
      "{'id': 0, 'image_id': 0, 'category_id': 2, 'bbox': [140, 262, 21, 25.5], 'area': 535.5, 'segmentation': [], 'iscrowd': 0}\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print (\"--------------------------------------------------------\")\n",
    "# Check the structure of the JSON file\n",
    "print(\"Keys in the COCO annotations:\", list(coco.dataset.keys()))\n",
    "\n",
    "print (\"--------------------------------------------------------\")\n",
    "print (\"--------------------------------------------------------\")\n",
    "# Total number of images\n",
    "num_images = len(coco.getImgIds())\n",
    "print(f\"Total images in the dataset: {num_images}\")\n",
    "\n",
    "# Total number of annotations\n",
    "num_annotations = len(coco.getAnnIds())\n",
    "print(f\"Total annotations in the dataset: {num_annotations}\")\n",
    "\n",
    "# Total number of categories\n",
    "num_categories = len(coco.getCatIds())\n",
    "print(f\"Total categories in the dataset: {num_categories}\")\n",
    "\n",
    "# List all categories\n",
    "categories = coco.loadCats(coco.getCatIds())\n",
    "print(\"Categories:\", [cat['name'] for cat in categories])\n",
    "\n",
    "print (\"--------------------------------------------------------\")\n",
    "print (\"--------------------------------------------------------\")\n",
    "\n",
    "# Map image IDs to annotations\n",
    "img_to_anns = defaultdict(list)\n",
    "for ann in coco.loadAnns(coco.getAnnIds()):\n",
    "    img_to_anns[ann['image_id']].append(ann['category_id'])\n",
    "\n",
    "# Calculate the number of unique classes per image\n",
    "classes_per_image = [len(set(img_to_anns[img_id])) for img_id in coco.getImgIds()]\n",
    "\n",
    "# Statistics\n",
    "max_classes = max(classes_per_image)\n",
    "min_classes = min(classes_per_image)\n",
    "avg_classes = np.mean(classes_per_image)\n",
    "\n",
    "print(f\"Max classes per image: {max_classes}\")\n",
    "print(f\"Min classes per image: {min_classes}\")\n",
    "print(f\"Average classes per image: {avg_classes:.2f}\")\n",
    "\n",
    "print (\"--------------------------------------------------------\")\n",
    "print (\"--------------------------------------------------------\")\n",
    "# Example image entry\n",
    "sample_image_id = coco.getImgIds()[0]\n",
    "sample_image = coco.loadImgs(sample_image_id)[0]\n",
    "print(\"Sample image entry:\")\n",
    "print(sample_image)\n",
    "\n",
    "# Example annotation\n",
    "sample_ann_id = coco.getAnnIds(imgIds=sample_image_id)[0]\n",
    "sample_annotation = coco.loadAnns(sample_ann_id)[0]\n",
    "print(\"Sample annotation entry:\")\n",
    "print(sample_annotation)\n",
    "\n",
    "\n",
    "print (\"--------------------------------------------------------\")\n",
    "print (\"--------------------------------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 25330, Validation images: 2235, Testing images: 2235\n"
     ]
    }
   ],
   "source": [
    "# Dataset splits\n",
    "num_train = 25330\n",
    "num_val = 2235\n",
    "num_test = 2235\n",
    "\n",
    "# Shuffle and split the dataset\n",
    "all_image_ids = coco.getImgIds()\n",
    "random.seed(42)  # Ensure reproducibility\n",
    "random.shuffle(all_image_ids)\n",
    "\n",
    "train_ids = all_image_ids[:num_train]\n",
    "val_ids = all_image_ids[num_train:num_train + num_val]\n",
    "test_ids = all_image_ids[num_train + num_val:num_train + num_val + num_test]\n",
    "\n",
    "print(f\"Training images: {len(train_ids)}, Validation images: {len(val_ids)}, Testing images: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom COCO dataset\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, image_dir, coco, processor, image_ids):\n",
    "        self.image_dir = image_dir\n",
    "        self.coco = coco\n",
    "        self.processor = processor\n",
    "        self.image_ids = image_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.image_dir, image_info[\"file_name\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Get annotations\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        annotations = []\n",
    "\n",
    "        for ann in anns:\n",
    "            bbox = ann[\"bbox\"]  # [x_min, y_min, width, height]\n",
    "            area = bbox[2] * bbox[3]  # width * height\n",
    "            annotations.append({\n",
    "                \"bbox\": bbox,\n",
    "                \"category_id\": ann[\"category_id\"],  # Preserve original category IDs\n",
    "                \"area\": area,\n",
    "                \"iscrowd\": ann.get(\"iscrowd\", 0),\n",
    "            })\n",
    "\n",
    "        # Prepare the target dictionary\n",
    "        target = {\"image_id\": image_id, \"annotations\": annotations}\n",
    "\n",
    "        # Process the image and annotations\n",
    "        encoding = self.processor(images=image, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze(0)\n",
    "        labels = encoding[\"labels\"][0]\n",
    "        return pixel_values, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets prepared: 25330 training, 2235 validation, 2235 testing.\n"
     ]
    }
   ],
   "source": [
    "# Path to images\n",
    "image_dir = data_folder\n",
    "\n",
    "# Load pre-trained processor\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "# Create datasets for each split\n",
    "train_dataset = COCODataset(image_dir, coco, processor, train_ids)\n",
    "val_dataset = COCODataset(image_dir, coco, processor, val_ids)\n",
    "test_dataset = COCODataset(image_dir, coco, processor, test_ids)\n",
    "\n",
    "print(f\"Datasets prepared: {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders ready.\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "print(\"DataLoaders ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values shape: torch.Size([3, 800, 800])\n",
      "Target example: {'size': tensor([800, 800]), 'image_id': tensor([6152]), 'class_labels': tensor([2, 2, 5, 5]), 'boxes': tensor([[0.3301, 0.5371, 0.0977, 0.1367],\n",
      "        [0.4155, 0.5581, 0.0498, 0.0732],\n",
      "        [0.6660, 0.3540, 0.0430, 0.0869],\n",
      "        [0.6670, 0.3301, 0.0293, 0.1133]]), 'area': tensor([8544.9219, 2334.5947, 2390.1367, 2124.0234]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([512, 512])}\n"
     ]
    }
   ],
   "source": [
    "# Test a batch\n",
    "for pixel_values, targets in train_loader:\n",
    "    print(f\"Pixel values shape: {pixel_values[0].shape}\")\n",
    "    print(f\"Target example: {targets[0]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DetrForObjectDetection(\n",
       "  (model): DetrModel(\n",
       "    (backbone): DetrConvModel(\n",
       "      (conv_encoder): DetrConvEncoder(\n",
       "        (model): FeatureListNet(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): DetrFrozenBatchNorm2d()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (layer1): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer2): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer3): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (4): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (5): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer4): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (position_embedding): DetrSinePositionEmbedding()\n",
       "    )\n",
       "    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (query_position_embeddings): Embedding(100, 256)\n",
       "    (encoder): DetrEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetrEncoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): DetrDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetrDecoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (class_labels_classifier): Linear(in_features=256, out_features=13, bias=True)\n",
       "  (bbox_predictor): DetrMLPPredictionHead(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model and processor\n",
    "model_path = \"DETR_FullNetworkTrained_80epochs/detr-finetuned\"\n",
    "processor_path = \"DETR_FullNetworkTrained_80epochs/detr-processor-stage6\"\n",
    "\n",
    "model = DetrForObjectDetection.from_pretrained(model_path)\n",
    "processor = DetrImageProcessor.from_pretrained(processor_path)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "## *FOR CALCULATING MEAN IOU OF TEST PREDICTIONS*\n",
    "\n",
    "def calculate_mean_iou(test_predictions, test_ground_truths):\n",
    "    all_ious = []\n",
    "    \n",
    "    for pred_sample, gt_sample in zip(test_predictions, test_ground_truths):\n",
    "        pred_boxes = pred_sample['boxes']\n",
    "        pred_scores = pred_sample['scores']\n",
    "        gt_boxes = gt_sample['boxes']\n",
    "        \n",
    "        if pred_boxes.size==0:  # Skip if no predictions\n",
    "            continue\n",
    "            \n",
    "        # Sort predictions by confidence score\n",
    "        pred_boxes_with_scores = sorted(zip(pred_boxes, pred_scores), \n",
    "                                      key=lambda x: x[1], \n",
    "                                      reverse=True)\n",
    "        pred_boxes = [box for box, _ in pred_boxes_with_scores]\n",
    "        \n",
    "        # Calculate best IoU for each ground truth box\n",
    "        for gt_box in gt_boxes:\n",
    "            ious = [calculate_iou(gt_box, pred_box) for pred_box in pred_boxes]\n",
    "            if ious:\n",
    "                max_iou = max(ious)\n",
    "                all_ious.append(max_iou)\n",
    "    \n",
    "    # Calculate mean IoU\n",
    "    mean_iou = sum(all_ious) / len(all_ious) if all_ious else 0.0\n",
    "    return mean_iou\n",
    "\n",
    "##*FOR CALCULATING MEAN IOU OF TEST PREDICTIONS PER CLASS*\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_mean_iou_per_class(test_predictions, test_ground_truths):\n",
    "    iou_per_class = defaultdict(list)\n",
    "    \n",
    "    for pred_sample, gt_sample in zip(test_predictions, test_ground_truths):\n",
    "        pred_boxes = pred_sample['boxes']\n",
    "        pred_labels = pred_sample['labels']\n",
    "        pred_scores = pred_sample['scores']\n",
    "        gt_boxes = gt_sample['boxes']\n",
    "        gt_labels = gt_sample['labels']\n",
    "        \n",
    "        # Group ground truth boxes by class\n",
    "        gt_boxes_by_class = defaultdict(list)\n",
    "        for gt_box, gt_label in zip(gt_boxes, gt_labels):\n",
    "            gt_boxes_by_class[gt_label].append(gt_box)\n",
    "            \n",
    "        # Group predicted boxes by class\n",
    "        pred_boxes_by_class = defaultdict(list)\n",
    "        for pred_box, pred_label, pred_score in zip(pred_boxes, pred_labels, pred_scores):\n",
    "            pred_boxes_by_class[pred_label].append((pred_box, pred_score))\n",
    "            \n",
    "        # Calculate IoU for each class\n",
    "        for class_id in gt_boxes_by_class.keys():\n",
    "            gt_boxes_class = gt_boxes_by_class[class_id]\n",
    "            pred_boxes_class = pred_boxes_by_class[class_id]\n",
    "            \n",
    "            if not pred_boxes_class:  # No predictions for this class\n",
    "                continue\n",
    "                \n",
    "            # Sort predictions by confidence score\n",
    "            pred_boxes_class = sorted(pred_boxes_class, key=lambda x: x[1], reverse=True)\n",
    "            pred_boxes_only = [box for box, _ in pred_boxes_class]\n",
    "            \n",
    "            # Calculate best IoU for each ground truth box\n",
    "            for gt_box in gt_boxes_class:\n",
    "                ious = [calculate_iou(gt_box, pred_box) for pred_box in pred_boxes_only]\n",
    "                if ious:\n",
    "                    max_iou = max(ious)\n",
    "                    iou_per_class[class_id].append(max_iou)\n",
    "    \n",
    "    # Calculate mean IoU for each class\n",
    "    mean_iou_per_class = {}\n",
    "    for class_id, ious in iou_per_class.items():\n",
    "        if ious:  # Only calculate mean if we have IoUs for this class\n",
    "            mean_iou_per_class[class_id] = sum(ious) / len(ious)\n",
    "        else:\n",
    "            mean_iou_per_class[class_id] = 0.0\n",
    "            \n",
    "    return mean_iou_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IoU: 0.0000\n",
      "Mean IoU per class:\n"
     ]
    }
   ],
   "source": [
    "# Perform evaluation on the test set\n",
    "test_predictions = []\n",
    "test_ground_truths = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for pixel_values, targets in test_loader:\n",
    "        # Move inputs to the appropriate device\n",
    "        pixel_values = torch.stack(pixel_values).to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Run model inference\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        \n",
    "        # Post-process outputs to get predictions in COCO format\n",
    "        target_sizes = torch.stack([t[\"orig_size\"] for t in targets]).to(device)\n",
    "        detections = processor.post_process_object_detection(\n",
    "            outputs, target_sizes=target_sizes, threshold=0.5\n",
    "        )\n",
    "        \n",
    "        # Convert detections to dictionary format\n",
    "        for idx, detection in enumerate(detections):\n",
    "            test_predictions.append({\n",
    "                \"boxes\": detection[\"boxes\"].cpu(),\n",
    "                \"scores\": detection[\"scores\"].cpu(),\n",
    "                \"labels\": detection[\"labels\"].cpu(),\n",
    "            })\n",
    "            test_ground_truths.append({\n",
    "                \"boxes\": targets[idx][\"boxes\"].cpu(),\n",
    "                \"labels\": targets[idx][\"class_labels\"].cpu(),\n",
    "            })\n",
    "\n",
    "# Evaluate the predictions\n",
    "mean_iou = calculate_mean_iou(test_predictions, test_ground_truths)\n",
    "print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "\n",
    "mean_iou_per_class = calculate_mean_iou_per_class(test_predictions, test_ground_truths)\n",
    "print(\"Mean IoU per class:\")\n",
    "for class_id, iou in mean_iou_per_class.items():\n",
    "    print(f\"Class {class_id}: {iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IoU with Libraries: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_mean_iou_with_libraries(predictions, ground_truths):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Intersection over Union (IoU) for a set of predictions and ground truths.\n",
    "\n",
    "    Args:\n",
    "        predictions (list of dict): List of dictionaries containing \"boxes\" (tensor) for predicted results.\n",
    "        ground_truths (list of dict): List of dictionaries containing \"boxes\" (tensor) for ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean IoU across all predictions and ground truths.\n",
    "    \"\"\"\n",
    "    def calculate_iou(box1, box2):\n",
    "        \"\"\"\n",
    "        Calculate IoU for two sets of boxes using Torch's box utilities.\n",
    "\n",
    "        Args:\n",
    "            box1 (tensor): Ground truth box in the format [x_min, y_min, x_max, y_max].\n",
    "            box2 (tensor): Predicted box in the format [x_min, y_min, x_max, y_max].\n",
    "\n",
    "        Returns:\n",
    "            tensor: Tensor of IoU values.\n",
    "        \"\"\"\n",
    "        # Compute intersection\n",
    "        x1 = torch.max(box1[0], box2[0])\n",
    "        y1 = torch.max(box1[1], box2[1])\n",
    "        x2 = torch.min(box1[2], box2[2])\n",
    "        y2 = torch.min(box1[3], box2[3])\n",
    "\n",
    "        intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n",
    "\n",
    "        # Compute areas\n",
    "        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "        # Compute union\n",
    "        union = area1 + area2 - intersection\n",
    "\n",
    "        # Avoid division by zero\n",
    "        iou = intersection / union if union > 0 else torch.tensor(0.0)\n",
    "\n",
    "        return iou\n",
    "\n",
    "    all_ious = []\n",
    "\n",
    "    # Loop over predictions and ground truths\n",
    "    for pred_sample, gt_sample in zip(predictions, ground_truths):\n",
    "        pred_boxes = pred_sample[\"boxes\"]\n",
    "        gt_boxes = gt_sample[\"boxes\"]\n",
    "\n",
    "        if len(pred_boxes) == 0 or len(gt_boxes) == 0:\n",
    "            continue\n",
    "\n",
    "        # Match each ground truth box with the predicted boxes\n",
    "        for gt_box in gt_boxes:\n",
    "            ious = [calculate_iou(gt_box, pred_box) for pred_box in pred_boxes]\n",
    "            if ious:  # Append the highest IoU for this ground truth box\n",
    "                max_iou = max(ious)\n",
    "                all_ious.append(max_iou.item())\n",
    "\n",
    "    # Compute the mean IoU\n",
    "    mean_iou = sum(all_ious) / len(all_ious) if all_ious else 0.0\n",
    "    return mean_iou\n",
    "\n",
    "# Assuming `test_predictions` and `test_ground_truths` are prepared\n",
    "mean_iou = calculate_mean_iou_with_libraries(test_predictions, test_ground_truths)\n",
    "print(f\"Mean IoU with Libraries: {mean_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'visualize_image_attr' from 'captum.attr' (/home/exouser/Downloads/my_env/lib/python3.12/site-packages/captum/attr/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaptum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Saliency, LayerGradCam, visualize_image_attr\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Given list of images with format: (index, file_name, label)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m image_info \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1478896951217198196_jpg.rf.9239d993f82a635b6b049a02b468f79a.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m5\u001b[39m),\n\u001b[1;32m     10\u001b[0m     (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1478900814278399033_jpg.rf.KeAPtrlh0ZRRk6ZiKs9Q.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     (\u001b[38;5;241m11\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1478901375856393827_jpg.rf.1S961wwKb7FA7YyHUFQN.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     20\u001b[0m ]\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'visualize_image_attr' from 'captum.attr' (/home/exouser/Downloads/my_env/lib/python3.12/site-packages/captum/attr/__init__.py)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "from captum.attr import Saliency, LayerGradCam, visualize_image_attr\n",
    "\n",
    "# Given list of images with format: (index, file_name, label)\n",
    "image_info = [\n",
    "    (1, '1478896951217198196_jpg.rf.9239d993f82a635b6b049a02b468f79a.jpg', 5),\n",
    "    (2, '1478900814278399033_jpg.rf.KeAPtrlh0ZRRk6ZiKs9Q.jpg', 0),\n",
    "    (3, '1478899822519346516_jpg.rf.0CaEssvbf3X8EhzEcKYy.jpg', 1),\n",
    "    (4, '1478900814278399033_jpg.rf.KeAPtrlh0ZRRk6ZiKs9Q.jpg', 0),\n",
    "    (5, '1478900814278399033_jpg.rf.KeAPtrlh0ZRRk6ZiKs9Q.jpg', 0),\n",
    "    (6, '1478898675369279101_jpg.rf.45446effd380efe9c27f68196331170e.jpg', 6),\n",
    "    (7, '1478899822519346516_jpg.rf.0CaEssvbf3X8EhzEcKYy.jpg', 1),\n",
    "    (8, '1478900133301593773_jpg.rf.c625f53c7db5c81e4c47a5fba26ce988.jpg', 7),\n",
    "    (9, '1478896940362299188_jpg.rf.g5KbYm7iOwkZwHlr0VMU.jpg', 171),\n",
    "    (10, '1478900373242343356_jpg.rf.f736929a171351498ab012571e1de639.jpg', 441),\n",
    "    (11, '1478901375856393827_jpg.rf.1S961wwKb7FA7YyHUFQN.jpg', 2),\n",
    "]\n",
    "\n",
    "# Path to the test set folder\n",
    "test_image_dir = \"Self Driving Car/export/\"\n",
    "\n",
    "# Function to load the test images\n",
    "def load_test_images(image_info, processor):\n",
    "    test_images = []\n",
    "    file_paths = []\n",
    "    for idx, file_name, label in image_info:\n",
    "        file_path = os.path.join(test_image_dir, file_name)\n",
    "        image = Image.open(file_path).convert(\"RGB\")\n",
    "        processed_image = processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "        test_images.append((processed_image, label))\n",
    "        file_paths.append(file_path)\n",
    "    return test_images, file_paths\n",
    "\n",
    "# Load the processor and test images\n",
    "processor = DetrImageProcessor.from_pretrained(\"DETR/detr-processor\")\n",
    "test_images, file_paths = load_test_images(image_info, processor)\n",
    "\n",
    "# Function to plot Grad-CAM and Saliency Maps\n",
    "def plot_grad_cam_and_saliency_maps(model, test_images, file_paths):\n",
    "    \"\"\"\n",
    "    Plot Grad-CAM and saliency maps for a list of test images.\n",
    "\n",
    "    Args:\n",
    "        model: The trained DETR model.\n",
    "        test_images: List of tuples containing (processed_image, label).\n",
    "        file_paths: List of file paths for the original images.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    grad_cam = LayerGradCam(model, model.model.backbone.conv_encoder.model.layer4)\n",
    "    saliency = Saliency(model)\n",
    "\n",
    "    for idx, (processed_image, file_path) in enumerate(zip(test_images, file_paths)):\n",
    "        processed_image_tensor = processed_image[0].unsqueeze(0).to(device)\n",
    "        original_label = processed_image[1]\n",
    "\n",
    "        # Grad-CAM\n",
    "        grad_cam_attr = grad_cam.attribute(processed_image_tensor, target=original_label)\n",
    "        grad_cam_attr = grad_cam_attr.squeeze().cpu().detach().numpy()\n",
    "\n",
    "        # Saliency Map\n",
    "        saliency_attr = saliency.attribute(processed_image_tensor, target=original_label)\n",
    "        saliency_attr = saliency_attr.squeeze().cpu().detach().numpy()\n",
    "\n",
    "        # Load and display the original image\n",
    "        original_image = Image.open(file_path).convert(\"RGB\")\n",
    "\n",
    "        # Visualization\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(20, 8))\n",
    "        axs[0].imshow(original_image)\n",
    "        axs[0].set_title(f\"Original Image: {file_path.split('/')[-1]}\")\n",
    "        axs[0].axis(\"off\")\n",
    "\n",
    "        axs[1].imshow(grad_cam_attr, cmap=\"jet\")\n",
    "        axs[1].set_title(\"Grad-CAM\")\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        axs[2].imshow(saliency_attr, cmap=\"hot\")\n",
    "        axs[2].set_title(\"Saliency Map\")\n",
    "        axs[2].axis(\"off\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Load the model\n",
    "model = DetrForObjectDetection.from_pretrained(\"DETR/detr-finetuned\")\n",
    "\n",
    "# Plot Grad-CAM and Saliency Maps\n",
    "plot_grad_cam_and_saliency_maps(model, test_images, file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
