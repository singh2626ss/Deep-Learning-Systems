{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETR Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/Downloads/my_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-15 13:14:55.875737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734268495.893785  382538 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734268495.899202  382538 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 13:14:55.919547: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import time  # For measuring time\n",
    "\n",
    "# Third-party imports\n",
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PIL (Python Imaging Library)\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# torchvision imports\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import DetrForObjectDetection, DetrConfig, DetrImageProcessor\n",
    "\n",
    "# pycocotools imports\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# timm (PyTorch Image Models) import\n",
    "import timm\n",
    "\n",
    "# Collections\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.79s)\n",
      "creating index...\n",
      "index created!\n",
      "Total images in the dataset: 29800\n",
      "Keys in the COCO annotations: ['info', 'licenses', 'categories', 'images', 'annotations']\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "data_folder = \"Self Driving Car/export/\"\n",
    "annotation_file = os.path.join(data_folder, \"_annotations.coco.json\")\n",
    "\n",
    "# Load COCO Annotations\n",
    "coco = COCO(annotation_file)\n",
    "\n",
    "# Print dataset statistics\n",
    "total_images = len(coco.getImgIds())\n",
    "print(f\"Total images in the dataset: {total_images}\")\n",
    "print(\"Keys in the COCO annotations:\", list(coco.dataset.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Inspect the annotation file to make sure it is suitable fro DETR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Keys in the COCO annotations: ['info', 'licenses', 'categories', 'images', 'annotations']\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Total images in the dataset: 29800\n",
      "Total annotations in the dataset: 194539\n",
      "Total categories in the dataset: 12\n",
      "Categories: ['obstacles', 'biker', 'car', 'pedestrian', 'trafficLight', 'trafficLight-Green', 'trafficLight-GreenLeft', 'trafficLight-Red', 'trafficLight-RedLeft', 'trafficLight-Yellow', 'trafficLight-YellowLeft', 'truck']\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Max classes per image: 7\n",
      "Min classes per image: 0\n",
      "Average classes per image: 1.88\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Sample image entry:\n",
      "{'id': 0, 'license': 1, 'file_name': '1478897026627294725_jpg.rf.6828a4e821cbab4c2c277d74df291f00.jpg', 'height': 512, 'width': 512, 'date_captured': '2021-06-09T12:24:25+00:00'}\n",
      "Sample annotation entry:\n",
      "{'id': 0, 'image_id': 0, 'category_id': 2, 'bbox': [140, 262, 21, 25.5], 'area': 535.5, 'segmentation': [], 'iscrowd': 0}\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print (\"--------------------------------------------------------\")\n",
    "# Check the structure of the JSON file\n",
    "print(\"Keys in the COCO annotations:\", list(coco.dataset.keys()))\n",
    "\n",
    "print (\"--------------------------------------------------------\")\n",
    "print (\"--------------------------------------------------------\")\n",
    "# Total number of images\n",
    "num_images = len(coco.getImgIds())\n",
    "print(f\"Total images in the dataset: {num_images}\")\n",
    "\n",
    "# Total number of annotations\n",
    "num_annotations = len(coco.getAnnIds())\n",
    "print(f\"Total annotations in the dataset: {num_annotations}\")\n",
    "\n",
    "# Total number of categories\n",
    "num_categories = len(coco.getCatIds())\n",
    "print(f\"Total categories in the dataset: {num_categories}\")\n",
    "\n",
    "# List all categories\n",
    "categories = coco.loadCats(coco.getCatIds())\n",
    "print(\"Categories:\", [cat['name'] for cat in categories])\n",
    "\n",
    "print (\"--------------------------------------------------------\")\n",
    "print (\"--------------------------------------------------------\")\n",
    "\n",
    "# Map image IDs to annotations\n",
    "img_to_anns = defaultdict(list)\n",
    "for ann in coco.loadAnns(coco.getAnnIds()):\n",
    "    img_to_anns[ann['image_id']].append(ann['category_id'])\n",
    "\n",
    "# Calculate the number of unique classes per image\n",
    "classes_per_image = [len(set(img_to_anns[img_id])) for img_id in coco.getImgIds()]\n",
    "\n",
    "# Statistics\n",
    "max_classes = max(classes_per_image)\n",
    "min_classes = min(classes_per_image)\n",
    "avg_classes = np.mean(classes_per_image)\n",
    "\n",
    "print(f\"Max classes per image: {max_classes}\")\n",
    "print(f\"Min classes per image: {min_classes}\")\n",
    "print(f\"Average classes per image: {avg_classes:.2f}\")\n",
    "\n",
    "print (\"--------------------------------------------------------\")\n",
    "print (\"--------------------------------------------------------\")\n",
    "# Example image entry\n",
    "sample_image_id = coco.getImgIds()[0]\n",
    "sample_image = coco.loadImgs(sample_image_id)[0]\n",
    "print(\"Sample image entry:\")\n",
    "print(sample_image)\n",
    "\n",
    "# Example annotation\n",
    "sample_ann_id = coco.getAnnIds(imgIds=sample_image_id)[0]\n",
    "sample_annotation = coco.loadAnns(sample_ann_id)[0]\n",
    "print(\"Sample annotation entry:\")\n",
    "print(sample_annotation)\n",
    "\n",
    "\n",
    "print (\"--------------------------------------------------------\")\n",
    "print (\"--------------------------------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 25330, Validation images: 2235, Testing images: 2235\n"
     ]
    }
   ],
   "source": [
    "# Dataset splits\n",
    "num_train = 25330\n",
    "num_val = 2235\n",
    "num_test = 2235\n",
    "\n",
    "# Shuffle and split the dataset\n",
    "all_image_ids = coco.getImgIds()\n",
    "random.seed(42)  # Ensure reproducibility\n",
    "random.shuffle(all_image_ids)\n",
    "\n",
    "train_ids = all_image_ids[:num_train]\n",
    "val_ids = all_image_ids[num_train:num_train + num_val]\n",
    "test_ids = all_image_ids[num_train + num_val:num_train + num_val + num_test]\n",
    "\n",
    "print(f\"Training images: {len(train_ids)}, Validation images: {len(val_ids)}, Testing images: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom COCO dataset\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, image_dir, coco, processor, image_ids):\n",
    "        self.image_dir = image_dir\n",
    "        self.coco = coco\n",
    "        self.processor = processor\n",
    "        self.image_ids = image_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.image_dir, image_info[\"file_name\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Get annotations\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        annotations = []\n",
    "\n",
    "        for ann in anns:\n",
    "            bbox = ann[\"bbox\"]  # [x_min, y_min, width, height]\n",
    "            area = bbox[2] * bbox[3]  # width * height\n",
    "            annotations.append({\n",
    "                \"bbox\": bbox,\n",
    "                \"category_id\": ann[\"category_id\"],  # Preserve original category IDs\n",
    "                \"area\": area,\n",
    "                \"iscrowd\": ann.get(\"iscrowd\", 0),\n",
    "            })\n",
    "\n",
    "        # Prepare the target dictionary\n",
    "        target = {\"image_id\": image_id, \"annotations\": annotations}\n",
    "\n",
    "        # Process the image and annotations\n",
    "        encoding = self.processor(images=image, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze(0)\n",
    "        labels = encoding[\"labels\"][0]\n",
    "        return pixel_values, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets prepared: 25330 training, 2235 validation, 2235 testing.\n"
     ]
    }
   ],
   "source": [
    "# Path to images\n",
    "image_dir = data_folder\n",
    "\n",
    "# Load pre-trained processor\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "# Create datasets for each split\n",
    "train_dataset = COCODataset(image_dir, coco, processor, train_ids)\n",
    "val_dataset = COCODataset(image_dir, coco, processor, val_ids)\n",
    "test_dataset = COCODataset(image_dir, coco, processor, test_ids)\n",
    "\n",
    "print(f\"Datasets prepared: {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders ready.\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "print(\"DataLoaders ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values shape: torch.Size([3, 800, 800])\n",
      "Target example: {'size': tensor([800, 800]), 'image_id': tensor([29633]), 'class_labels': tensor([3, 3, 2, 7, 2, 2, 2, 2, 2, 4]), 'boxes': tensor([[0.0507, 0.5634, 0.0271, 0.1033],\n",
      "        [0.0521, 0.5618, 0.0260, 0.0767],\n",
      "        [0.1345, 0.5453, 0.0698, 0.0750],\n",
      "        [0.1379, 0.4164, 0.0219, 0.0633],\n",
      "        [0.1977, 0.5534, 0.0594, 0.0600],\n",
      "        [0.2003, 0.5259, 0.0490, 0.0400],\n",
      "        [0.3034, 0.5584, 0.0677, 0.0817],\n",
      "        [0.3732, 0.5485, 0.0354, 0.0383],\n",
      "        [0.5159, 0.5571, 0.0552, 0.0633],\n",
      "        [0.5954, 0.3854, 0.0229, 0.0717]]), 'area': tensor([1791.1111, 1277.7778, 3350.0000,  886.6667, 2280.0000, 1253.3334,\n",
      "        3538.8889,  868.8889, 2237.7778, 1051.1112]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'orig_size': tensor([512, 512])}\n"
     ]
    }
   ],
   "source": [
    "# Test a batch\n",
    "for pixel_values, targets in train_loader:\n",
    "    print(f\"Pixel values shape: {pixel_values[0].shape}\")\n",
    "    print(f\"Target example: {targets[0]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DetrForObjectDetection(\n",
       "  (model): DetrModel(\n",
       "    (backbone): DetrConvModel(\n",
       "      (conv_encoder): DetrConvEncoder(\n",
       "        (model): FeatureListNet(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): DetrFrozenBatchNorm2d()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (layer1): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer2): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer3): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (4): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (5): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer4): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (position_embedding): DetrSinePositionEmbedding()\n",
       "    )\n",
       "    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (query_position_embeddings): Embedding(100, 256)\n",
       "    (encoder): DetrEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetrEncoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): DetrDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetrDecoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (class_labels_classifier): Linear(in_features=256, out_features=13, bias=True)\n",
       "  (bbox_predictor): DetrMLPPredictionHead(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model and processor\n",
    "model_path = \"DETR/detr-finetuned\"\n",
    "processor_path = \"DETR/detr-processor\"\n",
    "\n",
    "model = DetrForObjectDetection.from_pretrained(model_path)\n",
    "processor = DetrImageProcessor.from_pretrained(processor_path)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "## *FOR CALCULATING MEAN IOU OF TEST PREDICTIONS*\n",
    "\n",
    "def calculate_mean_iou(test_predictions, test_ground_truths):\n",
    "    all_ious = []\n",
    "    \n",
    "    for pred_sample, gt_sample in zip(test_predictions, test_ground_truths):\n",
    "        pred_boxes = pred_sample['boxes']\n",
    "        pred_scores = pred_sample['scores']\n",
    "        gt_boxes = gt_sample['boxes']\n",
    "        \n",
    "        if pred_boxes.size==0:  # Skip if no predictions\n",
    "            continue\n",
    "            \n",
    "        # Sort predictions by confidence score\n",
    "        pred_boxes_with_scores = sorted(zip(pred_boxes, pred_scores), \n",
    "                                      key=lambda x: x[1], \n",
    "                                      reverse=True)\n",
    "        pred_boxes = [box for box, _ in pred_boxes_with_scores]\n",
    "        \n",
    "        # Calculate best IoU for each ground truth box\n",
    "        for gt_box in gt_boxes:\n",
    "            ious = [calculate_iou(gt_box, pred_box) for pred_box in pred_boxes]\n",
    "            if ious:\n",
    "                max_iou = max(ious)\n",
    "                all_ious.append(max_iou)\n",
    "    \n",
    "    # Calculate mean IoU\n",
    "    mean_iou = sum(all_ious) / len(all_ious) if all_ious else 0.0\n",
    "    return mean_iou\n",
    "\n",
    "##*FOR CALCULATING MEAN IOU OF TEST PREDICTIONS PER CLASS*\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_mean_iou_per_class(test_predictions, test_ground_truths):\n",
    "    iou_per_class = defaultdict(list)\n",
    "    \n",
    "    for pred_sample, gt_sample in zip(test_predictions, test_ground_truths):\n",
    "        pred_boxes = pred_sample['boxes']\n",
    "        pred_labels = pred_sample['labels']\n",
    "        pred_scores = pred_sample['scores']\n",
    "        gt_boxes = gt_sample['boxes']\n",
    "        gt_labels = gt_sample['labels']\n",
    "        \n",
    "        # Group ground truth boxes by class\n",
    "        gt_boxes_by_class = defaultdict(list)\n",
    "        for gt_box, gt_label in zip(gt_boxes, gt_labels):\n",
    "            gt_boxes_by_class[gt_label].append(gt_box)\n",
    "            \n",
    "        # Group predicted boxes by class\n",
    "        pred_boxes_by_class = defaultdict(list)\n",
    "        for pred_box, pred_label, pred_score in zip(pred_boxes, pred_labels, pred_scores):\n",
    "            pred_boxes_by_class[pred_label].append((pred_box, pred_score))\n",
    "            \n",
    "        # Calculate IoU for each class\n",
    "        for class_id in gt_boxes_by_class.keys():\n",
    "            gt_boxes_class = gt_boxes_by_class[class_id]\n",
    "            pred_boxes_class = pred_boxes_by_class[class_id]\n",
    "            \n",
    "            if not pred_boxes_class:  # No predictions for this class\n",
    "                continue\n",
    "                \n",
    "            # Sort predictions by confidence score\n",
    "            pred_boxes_class = sorted(pred_boxes_class, key=lambda x: x[1], reverse=True)\n",
    "            pred_boxes_only = [box for box, _ in pred_boxes_class]\n",
    "            \n",
    "            # Calculate best IoU for each ground truth box\n",
    "            for gt_box in gt_boxes_class:\n",
    "                ious = [calculate_iou(gt_box, pred_box) for pred_box in pred_boxes_only]\n",
    "                if ious:\n",
    "                    max_iou = max(ious)\n",
    "                    iou_per_class[class_id].append(max_iou)\n",
    "    \n",
    "    # Calculate mean IoU for each class\n",
    "    mean_iou_per_class = {}\n",
    "    for class_id, ious in iou_per_class.items():\n",
    "        if ious:  # Only calculate mean if we have IoUs for this class\n",
    "            mean_iou_per_class[class_id] = sum(ious) / len(ious)\n",
    "        else:\n",
    "            mean_iou_per_class[class_id] = 0.0\n",
    "            \n",
    "    return mean_iou_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_iou' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 33\u001b[0m\n\u001b[1;32m     27\u001b[0m             test_ground_truths\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     28\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m: targets[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu(),\n\u001b[1;32m     29\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: targets[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu(),\n\u001b[1;32m     30\u001b[0m             })\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Evaluate the predictions\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m mean_iou \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_mean_iou\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_ground_truths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean IoU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_iou\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m mean_iou_per_class \u001b[38;5;241m=\u001b[39m calculate_mean_iou_per_class(test_predictions, test_ground_truths)\n",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m, in \u001b[0;36mcalculate_mean_iou\u001b[0;34m(test_predictions, test_ground_truths)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Calculate best IoU for each ground truth box\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gt_box \u001b[38;5;129;01min\u001b[39;00m gt_boxes:\n\u001b[0;32m---> 24\u001b[0m     ious \u001b[38;5;241m=\u001b[39m [\u001b[43mcalculate_iou\u001b[49m(gt_box, pred_box) \u001b[38;5;28;01mfor\u001b[39;00m pred_box \u001b[38;5;129;01min\u001b[39;00m pred_boxes]\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ious:\n\u001b[1;32m     26\u001b[0m         max_iou \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(ious)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_iou' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform evaluation on the test set\n",
    "test_predictions = []\n",
    "test_ground_truths = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for pixel_values, targets in test_loader:\n",
    "        # Move inputs to the appropriate device\n",
    "        pixel_values = torch.stack(pixel_values).to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Run model inference\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        \n",
    "        # Post-process outputs to get predictions in COCO format\n",
    "        target_sizes = torch.stack([t[\"orig_size\"] for t in targets]).to(device)\n",
    "        detections = processor.post_process_object_detection(\n",
    "            outputs, target_sizes=target_sizes, threshold=0.5\n",
    "        )\n",
    "        \n",
    "        # Convert detections to dictionary format\n",
    "        for idx, detection in enumerate(detections):\n",
    "            test_predictions.append({\n",
    "                \"boxes\": detection[\"boxes\"].cpu(),\n",
    "                \"scores\": detection[\"scores\"].cpu(),\n",
    "                \"labels\": detection[\"labels\"].cpu(),\n",
    "            })\n",
    "            test_ground_truths.append({\n",
    "                \"boxes\": targets[idx][\"boxes\"].cpu(),\n",
    "                \"labels\": targets[idx][\"class_labels\"].cpu(),\n",
    "            })\n",
    "\n",
    "# Evaluate the predictions\n",
    "mean_iou = calculate_mean_iou(test_predictions, test_ground_truths)\n",
    "print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "\n",
    "mean_iou_per_class = calculate_mean_iou_per_class(test_predictions, test_ground_truths)\n",
    "print(\"Mean IoU per class:\")\n",
    "for class_id, iou in mean_iou_per_class.items():\n",
    "    print(f\"Class {class_id}: {iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IoU with Libraries: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_mean_iou_with_libraries(predictions, ground_truths):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Intersection over Union (IoU) for a set of predictions and ground truths.\n",
    "\n",
    "    Args:\n",
    "        predictions (list of dict): List of dictionaries containing \"boxes\" (tensor) for predicted results.\n",
    "        ground_truths (list of dict): List of dictionaries containing \"boxes\" (tensor) for ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean IoU across all predictions and ground truths.\n",
    "    \"\"\"\n",
    "    def calculate_iou(box1, box2):\n",
    "        \"\"\"\n",
    "        Calculate IoU for two sets of boxes using Torch's box utilities.\n",
    "\n",
    "        Args:\n",
    "            box1 (tensor): Ground truth box in the format [x_min, y_min, x_max, y_max].\n",
    "            box2 (tensor): Predicted box in the format [x_min, y_min, x_max, y_max].\n",
    "\n",
    "        Returns:\n",
    "            tensor: Tensor of IoU values.\n",
    "        \"\"\"\n",
    "        # Compute intersection\n",
    "        x1 = torch.max(box1[0], box2[0])\n",
    "        y1 = torch.max(box1[1], box2[1])\n",
    "        x2 = torch.min(box1[2], box2[2])\n",
    "        y2 = torch.min(box1[3], box2[3])\n",
    "\n",
    "        intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n",
    "\n",
    "        # Compute areas\n",
    "        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "        # Compute union\n",
    "        union = area1 + area2 - intersection\n",
    "\n",
    "        # Avoid division by zero\n",
    "        iou = intersection / union if union > 0 else torch.tensor(0.0)\n",
    "\n",
    "        return iou\n",
    "\n",
    "    all_ious = []\n",
    "\n",
    "    # Loop over predictions and ground truths\n",
    "    for pred_sample, gt_sample in zip(predictions, ground_truths):\n",
    "        pred_boxes = pred_sample[\"boxes\"]\n",
    "        gt_boxes = gt_sample[\"boxes\"]\n",
    "\n",
    "        if len(pred_boxes) == 0 or len(gt_boxes) == 0:\n",
    "            continue\n",
    "\n",
    "        # Match each ground truth box with the predicted boxes\n",
    "        for gt_box in gt_boxes:\n",
    "            ious = [calculate_iou(gt_box, pred_box) for pred_box in pred_boxes]\n",
    "            if ious:  # Append the highest IoU for this ground truth box\n",
    "                max_iou = max(ious)\n",
    "                all_ious.append(max_iou.item())\n",
    "\n",
    "    # Compute the mean IoU\n",
    "    mean_iou = sum(all_ious) / len(all_ious) if all_ious else 0.0\n",
    "    return mean_iou\n",
    "\n",
    "# Assuming `test_predictions` and `test_ground_truths` are prepared\n",
    "mean_iou = calculate_mean_iou_with_libraries(test_predictions, test_ground_truths)\n",
    "print(f\"Mean IoU with Libraries: {mean_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m model \u001b[38;5;241m=\u001b[39m DetrForObjectDetection\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDETR/detr-finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Plot Grad-CAM and Saliency Maps\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m plot_grad_cam_and_saliency_maps(model, \u001b[43mtest_images\u001b[49m, file_paths)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_images' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the model\n",
    "model = DetrForObjectDetection.from_pretrained(\"DETR/detr-finetuned\")\n",
    "\n",
    "# Plot Grad-CAM and Saliency Maps\n",
    "plot_grad_cam_and_saliency_maps(model, test_images, file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DetrObjectDetectionOutput' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m model \u001b[38;5;241m=\u001b[39m DetrForObjectDetection\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDETR/detr-finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Plot Grad-CAM and Saliency Maps\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m \u001b[43mplot_grad_cam_and_saliency_maps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 95\u001b[0m, in \u001b[0;36mplot_grad_cam_and_saliency_maps\u001b[0;34m(model, test_images, file_paths)\u001b[0m\n\u001b[1;32m     92\u001b[0m original_label \u001b[38;5;241m=\u001b[39m processed_image[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Grad-CAM\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m grad_cam_attr \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_cam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_image_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m grad_cam_attr \u001b[38;5;241m=\u001b[39m grad_cam_attr\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Saliency Map\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/my_env/lib/python3.12/site-packages/captum/log/__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/my_env/lib/python3.12/site-packages/captum/attr/_core/layer/grad_cam.py:195\u001b[0m, in \u001b[0;36mLayerGradCam.attribute\u001b[0;34m(self, inputs, target, additional_forward_args, attribute_to_layer_input, relu_attributions, attr_dim_summation)\u001b[0m\n\u001b[1;32m    190\u001b[0m additional_forward_args \u001b[38;5;241m=\u001b[39m _format_additional_forward_args(\n\u001b[1;32m    191\u001b[0m     additional_forward_args\n\u001b[1;32m    192\u001b[0m )\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Returns gradient of output with respect to\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# hidden layer and hidden layer evaluated at each input.\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m layer_gradients, layer_evals \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_layer_gradients_and_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m summed_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    206\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmean(\n\u001b[1;32m    207\u001b[0m         layer_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer_grad \u001b[38;5;129;01min\u001b[39;00m layer_gradients\n\u001b[1;32m    214\u001b[0m )\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr_dim_summation:\n",
      "File \u001b[0;32m~/Downloads/my_env/lib/python3.12/site-packages/captum/_utils/gradient.py:592\u001b[0m, in \u001b[0;36mcompute_layer_gradients_and_eval\u001b[0;34m(forward_fn, layer, inputs, target_ind, additional_forward_args, gradient_neuron_selector, device_ids, attribute_to_layer_input, output_fn)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03mComputes gradients of the output with respect to a given layer as well\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03mas the output evaluation of the layer for an arbitrary forward function\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;124;03m        Target layer output for given input.\u001b[39;00m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;66;03m# saved_layer is a dictionary mapping device to a tuple of\u001b[39;00m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;66;03m# layer evaluations on that device.\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m     saved_layer, output \u001b[38;5;241m=\u001b[39m \u001b[43m_forward_layer_distributed_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforward_hook_with_return\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequire_layer_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, (\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget not provided when necessary, cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m take gradient with respect to multiple outputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    607\u001b[0m     device_ids \u001b[38;5;241m=\u001b[39m _extract_device_ids(forward_fn, saved_layer, device_ids)\n",
      "File \u001b[0;32m~/Downloads/my_env/lib/python3.12/site-packages/captum/_utils/gradient.py:294\u001b[0m, in \u001b[0;36m_forward_layer_distributed_eval\u001b[0;34m(forward_fn, inputs, layer, target_ind, additional_forward_args, attribute_to_layer_input, forward_hook_with_return, require_layer_grads)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m             all_hooks\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    292\u001b[0m                 single_layer\u001b[38;5;241m.\u001b[39mregister_forward_hook(hook_wrapper(single_layer))\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m all_hooks:\n",
      "File \u001b[0;32m~/Downloads/my_env/lib/python3.12/site-packages/captum/_utils/common.py:536\u001b[0m, in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    529\u001b[0m additional_forward_args \u001b[38;5;241m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[1;32m    531\u001b[0m output \u001b[38;5;241m=\u001b[39m forward_func(\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39madditional_forward_args)\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m additional_forward_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m inputs\n\u001b[1;32m    535\u001b[0m )\n\u001b[0;32m--> 536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_select_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/my_env/lib/python3.12/site-packages/captum/_utils/common.py:580\u001b[0m, in \u001b[0;36m_select_targets\u001b[0;34m(output, target)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m--> 580\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    581\u001b[0m dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    582\u001b[0m device \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DetrObjectDetectionOutput' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from captum.attr import Saliency, LayerGradCam\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# Given list of images with format: (index, file_name, label)\n",
    "image_info = [\n",
    "    (1, '1478896951217198196_jpg.rf.9239d993f82a635b6b049a02b468f79a.jpg', 5),\n",
    "    (2, '1478900814278399033_jpg.rf.KeAPtrlh0ZRRk6ZiKs9Q.jpg', 0),\n",
    "    (3, '1478899822519346516_jpg.rf.0CaEssvbf3X8EhzEcKYy.jpg', 1),\n",
    "    (4, '1478900814278399033_jpg.rf.KeAPtrlh0ZRRk6ZiKs9Q.jpg', 0),\n",
    "    (5, '1478900814278399033_jpg.rf.KeAPtrlh0ZRRk6ZiKs9Q.jpg', 0),\n",
    "    (6, '1478898675369279101_jpg.rf.45446effd380efe9c27f68196331170e.jpg', 6),\n",
    "    (7, '1478899822519346516_jpg.rf.0CaEssvbf3X8EhzEcKYy.jpg', 1),\n",
    "    (8, '1478900133301593773_jpg.rf.c625f53c7db5c81e4c47a5fba26ce988.jpg', 7),\n",
    "    (9, '1478896940362299188_jpg.rf.g5KbYm7iOwkZwHlr0VMU.jpg', 171),\n",
    "    (10, '1478900373242343356_jpg.rf.f736929a171351498ab012571e1de639.jpg', 441),\n",
    "    (11, '1478901375856393827_jpg.rf.1S961wwKb7FA7YyHUFQN.jpg', 2),\n",
    "]\n",
    "\n",
    "# Path to the test set folder\n",
    "test_image_dir = \"Self Driving Car/export/\"\n",
    "\n",
    "# Function to load the test images\n",
    "def load_test_images(image_info, processor):\n",
    "    test_images = []\n",
    "    file_paths = []\n",
    "    for idx, file_name, label in image_info:\n",
    "        file_path = os.path.join(test_image_dir, file_name)\n",
    "        image = Image.open(file_path).convert(\"RGB\")\n",
    "        processed_image = processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "        test_images.append((processed_image, label))\n",
    "        file_paths.append(file_path)\n",
    "    return test_images, file_paths\n",
    "\n",
    "# Load the processor and test images\n",
    "processor = DetrImageProcessor.from_pretrained(\"DETR/detr-processor\")\n",
    "test_images, file_paths = load_test_images(image_info, processor)\n",
    "\n",
    "# Custom visualization function\n",
    "def visualize_attributions(original_image, grad_cam_attr, saliency_attr, title=\"Visualization\"):\n",
    "    \"\"\"\n",
    "    Visualizes the Grad-CAM and Saliency maps side by side with the original image.\n",
    "\n",
    "    Args:\n",
    "        original_image: Original PIL image to display.\n",
    "        grad_cam_attr: Grad-CAM attribution map.\n",
    "        saliency_attr: Saliency attribution map.\n",
    "        title: Title for the entire visualization.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "    # Original Image\n",
    "    axs[0].imshow(original_image)\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    # Grad-CAM Visualization\n",
    "    axs[1].imshow(grad_cam_attr, cmap=\"jet\")\n",
    "    axs[1].set_title(\"Grad-CAM\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    # Saliency Map Visualization\n",
    "    axs[2].imshow(saliency_attr, cmap=\"hot\")\n",
    "    axs[2].set_title(\"Saliency Map\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot Grad-CAM and Saliency Maps\n",
    "def plot_grad_cam_and_saliency_maps(model, test_images, file_paths):\n",
    "    \"\"\"\n",
    "    Plot Grad-CAM and saliency maps for a list of test images.\n",
    "\n",
    "    Args:\n",
    "        model: The trained DETR model.\n",
    "        test_images: List of tuples containing (processed_image, label).\n",
    "        file_paths: List of file paths for the original images.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    grad_cam = LayerGradCam(model, model.model.backbone.conv_encoder.model.layer4)\n",
    "    saliency = Saliency(model)\n",
    "\n",
    "    for idx, (processed_image, file_path) in enumerate(zip(test_images, file_paths)):\n",
    "        processed_image_tensor = processed_image[0].unsqueeze(0).to(device)\n",
    "        original_label = processed_image[1]\n",
    "\n",
    "        # Grad-CAM\n",
    "        grad_cam_attr = grad_cam.attribute(processed_image_tensor, target=original_label)\n",
    "        grad_cam_attr = grad_cam_attr.squeeze().cpu().detach().numpy()\n",
    "\n",
    "        # Saliency Map\n",
    "        saliency_attr = saliency.attribute(processed_image_tensor, target=original_label)\n",
    "        saliency_attr = saliency_attr.squeeze().cpu().detach().numpy()\n",
    "\n",
    "        # Load and display the original image\n",
    "        original_image = Image.open(file_path).convert(\"RGB\")\n",
    "\n",
    "        # Visualization\n",
    "        visualize_attributions(original_image, grad_cam_attr, saliency_attr, title=f\"Image {idx + 1}\")\n",
    "\n",
    "        \n",
    "# Load the model\n",
    "model = DetrForObjectDetection.from_pretrained(\"DETR/detr-finetuned\")\n",
    "\n",
    "# Plot Grad-CAM and Saliency Maps\n",
    "plot_grad_cam_and_saliency_maps(model, test_images, file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
