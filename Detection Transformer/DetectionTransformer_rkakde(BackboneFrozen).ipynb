{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Deep Learning Systems\n",
    "## Detection Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/Downloads/my_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-11 06:53:59.754885: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733900039.776358   79718 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733900039.783117   79718 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-11 06:53:59.809735: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import time  # For measuring time\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import DetrForObjectDetection, DetrConfig, DetrImageProcessor\n",
    "\n",
    "# torchvision imports\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# pycocotools imports\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Matplotlib imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# timm (PyTorch Image Models) import\n",
    "import timm\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import DetrImageProcessor\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.80s)\n",
      "creating index...\n",
      "index created!\n",
      "Total images in the dataset: 29800\n",
      "Keys in the COCO annotations: ['info', 'licenses', 'categories', 'images', 'annotations']\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "data_folder = \"Self Driving Car/export/\"\n",
    "annotation_file = os.path.join(data_folder, \"_annotations.coco.json\")\n",
    "\n",
    "# Load COCO Annotations\n",
    "coco = COCO(annotation_file)\n",
    "\n",
    "# Print dataset statistics\n",
    "total_images = len(coco.getImgIds())\n",
    "print(f\"Total images in the dataset: {total_images}\")\n",
    "print(\"Keys in the COCO annotations:\", list(coco.dataset.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Inspect the annotation file to make sure it is suitable fro DETR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Keys in the COCO annotations: ['info', 'licenses', 'categories', 'images', 'annotations']\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Total images in the dataset: 29800\n",
      "Total annotations in the dataset: 194539\n",
      "Total categories in the dataset: 12\n",
      "Categories: ['obstacles', 'biker', 'car', 'pedestrian', 'trafficLight', 'trafficLight-Green', 'trafficLight-GreenLeft', 'trafficLight-Red', 'trafficLight-RedLeft', 'trafficLight-Yellow', 'trafficLight-YellowLeft', 'truck']\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Max classes per image: 7\n",
      "Min classes per image: 0\n",
      "Average classes per image: 1.88\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Sample image entry:\n",
      "{'id': 0, 'license': 1, 'file_name': '1478897026627294725_jpg.rf.6828a4e821cbab4c2c277d74df291f00.jpg', 'height': 512, 'width': 512, 'date_captured': '2021-06-09T12:24:25+00:00'}\n",
      "Sample annotation entry:\n",
      "{'id': 0, 'image_id': 0, 'category_id': 2, 'bbox': [140, 262, 21, 25.5], 'area': 535.5, 'segmentation': [], 'iscrowd': 0}\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print (\"--------------------------------------------------------\")\n",
    "# Check the structure of the JSON file\n",
    "print(\"Keys in the COCO annotations:\", list(coco.dataset.keys()))\n",
    "\n",
    "print (\"--------------------------------------------------------\")\n",
    "print (\"--------------------------------------------------------\")\n",
    "# Total number of images\n",
    "num_images = len(coco.getImgIds())\n",
    "print(f\"Total images in the dataset: {num_images}\")\n",
    "\n",
    "# Total number of annotations\n",
    "num_annotations = len(coco.getAnnIds())\n",
    "print(f\"Total annotations in the dataset: {num_annotations}\")\n",
    "\n",
    "# Total number of categories\n",
    "num_categories = len(coco.getCatIds())\n",
    "print(f\"Total categories in the dataset: {num_categories}\")\n",
    "\n",
    "# List all categories\n",
    "categories = coco.loadCats(coco.getCatIds())\n",
    "print(\"Categories:\", [cat['name'] for cat in categories])\n",
    "\n",
    "print (\"--------------------------------------------------------\")\n",
    "print (\"--------------------------------------------------------\")\n",
    "\n",
    "# Map image IDs to annotations\n",
    "img_to_anns = defaultdict(list)\n",
    "for ann in coco.loadAnns(coco.getAnnIds()):\n",
    "    img_to_anns[ann['image_id']].append(ann['category_id'])\n",
    "\n",
    "# Calculate the number of unique classes per image\n",
    "classes_per_image = [len(set(img_to_anns[img_id])) for img_id in coco.getImgIds()]\n",
    "\n",
    "# Statistics\n",
    "max_classes = max(classes_per_image)\n",
    "min_classes = min(classes_per_image)\n",
    "avg_classes = np.mean(classes_per_image)\n",
    "\n",
    "print(f\"Max classes per image: {max_classes}\")\n",
    "print(f\"Min classes per image: {min_classes}\")\n",
    "print(f\"Average classes per image: {avg_classes:.2f}\")\n",
    "\n",
    "print (\"--------------------------------------------------------\")\n",
    "print (\"--------------------------------------------------------\")\n",
    "# Example image entry\n",
    "sample_image_id = coco.getImgIds()[0]\n",
    "sample_image = coco.loadImgs(sample_image_id)[0]\n",
    "print(\"Sample image entry:\")\n",
    "print(sample_image)\n",
    "\n",
    "# Example annotation\n",
    "sample_ann_id = coco.getAnnIds(imgIds=sample_image_id)[0]\n",
    "sample_annotation = coco.loadAnns(sample_ann_id)[0]\n",
    "print(\"Sample annotation entry:\")\n",
    "print(sample_annotation)\n",
    "\n",
    "\n",
    "print (\"--------------------------------------------------------\")\n",
    "print (\"--------------------------------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output confirms that the dataset is in a **COCO-compatible format** and is suitable for use with **DETR**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Key Details About the Dataset**\n",
    "- **Total Images**: 29,800\n",
    "- **Total Annotations**: 194,539\n",
    "- **Total Categories**: 12\n",
    "- **Categories**:\n",
    "  ```plaintext\n",
    "  ['obstacles', 'biker', 'car', 'pedestrian', 'trafficLight', \n",
    "   'trafficLight-Green', 'trafficLight-GreenLeft', 'trafficLight-Red',\n",
    "   'trafficLight-RedLeft', 'trafficLight-Yellow', 'trafficLight-YellowLeft', 'truck']\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Classes Per Image**\n",
    "- **Max Classes per Image**: 7\n",
    "  - Some images have up to 7 unique object categories.\n",
    "- **Min Classes per Image**: 0\n",
    "  - Some images have no objects annotated (annotations might be missing or labeled as empty).\n",
    "- **Average Classes per Image**: 1.88\n",
    "  - On average, each image contains nearly 2 unique categories.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Annotation Format (Sample Entry)**\n",
    "- **Image Entry**:\n",
    "  ```python\n",
    "  {'id': 0, 'license': 1, 'file_name': '1478897026627294725_jpg.rf.6828a4e821cbab4c2c277d74df291f00.jpg', \n",
    "   'height': 512, 'width': 512, 'date_captured': '2021-06-09T12:24:25+00:00'}\n",
    "  ```\n",
    "  - Contains `id`, `file_name`, and dimensions (`height` and `width`), which are essential for DETR training.\n",
    "\n",
    "- **Annotation Entry**:\n",
    "  ```python\n",
    "  {'id': 0, 'image_id': 0, 'category_id': 2, 'bbox': [140, 262, 21, 25.5], \n",
    "   'area': 535.5, 'segmentation': [], 'iscrowd': 0}\n",
    "  ```\n",
    "  - **`image_id`** links the annotation to the corresponding image.\n",
    "  - **`category_id`** identifies the object category.\n",
    "  - **`bbox`** is in the correct COCO format `[x_min, y_min, width, height]`.\n",
    "  - **`area`** is calculated (but not mandatory for DETR).\n",
    "  - **`iscrowd`** is set to `0`, which is the expected value for single objects (not groups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 25330, Validation images: 2235, Testing images: 2235\n"
     ]
    }
   ],
   "source": [
    "# Dataset splits\n",
    "num_train = 25330\n",
    "num_val = 2235\n",
    "num_test = 2235\n",
    "\n",
    "# Shuffle and split the dataset\n",
    "all_image_ids = coco.getImgIds()\n",
    "random.seed(42)  # Ensure reproducibility\n",
    "random.shuffle(all_image_ids)\n",
    "\n",
    "train_ids = all_image_ids[:num_train]\n",
    "val_ids = all_image_ids[num_train:num_train + num_val]\n",
    "test_ids = all_image_ids[num_train + num_val:num_train + num_val + num_test]\n",
    "\n",
    "print(f\"Training images: {len(train_ids)}, Validation images: {len(val_ids)}, Testing images: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom COCO dataset\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, image_dir, coco, processor, image_ids):\n",
    "        self.image_dir = image_dir\n",
    "        self.coco = coco\n",
    "        self.processor = processor\n",
    "        self.image_ids = image_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.image_dir, image_info[\"file_name\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Get annotations\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        annotations = []\n",
    "\n",
    "        for ann in anns:\n",
    "            bbox = ann[\"bbox\"]  # [x_min, y_min, width, height]\n",
    "            area = bbox[2] * bbox[3]  # width * height\n",
    "            annotations.append({\n",
    "                \"bbox\": bbox,\n",
    "                \"category_id\": ann[\"category_id\"],  # Preserve original category IDs\n",
    "                \"area\": area,\n",
    "                \"iscrowd\": ann.get(\"iscrowd\", 0),\n",
    "            })\n",
    "\n",
    "        # Prepare the target dictionary\n",
    "        target = {\"image_id\": image_id, \"annotations\": annotations}\n",
    "\n",
    "        # Process the image and annotations\n",
    "        encoding = self.processor(images=image, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze(0)\n",
    "        labels = encoding[\"labels\"][0]\n",
    "        return pixel_values, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets prepared: 25330 training, 2235 validation, 2235 testing.\n"
     ]
    }
   ],
   "source": [
    "# Path to images\n",
    "image_dir = data_folder\n",
    "\n",
    "# Load pre-trained processor\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "# Create datasets for each split\n",
    "train_dataset = COCODataset(image_dir, coco, processor, train_ids)\n",
    "val_dataset = COCODataset(image_dir, coco, processor, val_ids)\n",
    "test_dataset = COCODataset(image_dir, coco, processor, test_ids)\n",
    "\n",
    "print(f\"Datasets prepared: {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders ready.\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "print(\"DataLoaders ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values shape: torch.Size([3, 800, 800])\n",
      "Target example: {'size': tensor([800, 800]), 'image_id': tensor([3684]), 'class_labels': tensor([2, 2, 2, 2, 2, 2, 2]), 'boxes': tensor([[0.8896, 0.5536, 0.2207, 0.2792],\n",
      "        [0.3986, 0.4909, 0.0354, 0.0600],\n",
      "        [0.4305, 0.4890, 0.0406, 0.0600],\n",
      "        [0.5262, 0.4988, 0.0875, 0.1500],\n",
      "        [0.6637, 0.5074, 0.1125, 0.1750],\n",
      "        [0.7467, 0.4942, 0.0990, 0.0900],\n",
      "        [0.7479, 0.4912, 0.0896, 0.0683]]), 'area': tensor([39455.5547,  1360.0001,  1560.0000,  8400.0000, 12600.0000,  5700.0000,\n",
      "         3917.7778]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0]), 'orig_size': tensor([512, 512])}\n"
     ]
    }
   ],
   "source": [
    "# Test a batch\n",
    "for pixel_values, targets in train_loader:\n",
    "    print(f\"Pixel values shape: {pixel_values[0].shape}\")\n",
    "    print(f\"Target example: {targets[0]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup and Initial Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Modify the Classification Head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "Number of classes: 13\n"
     ]
    }
   ],
   "source": [
    "# Initialize DETR with a custom configuration\n",
    "num_classes = 12 # 12 object classes + 1 background (created automatically)\n",
    "config = DetrConfig(num_labels=num_classes,ignore_mismatched_sizes=True, backbone=\"resnet50\")\n",
    "model = DetrForObjectDetection(config)\n",
    "\n",
    "# Create the Backbone Using timm\n",
    "# Create the pre-trained timm ResNet-50 backbone\n",
    "timm_backbone = timm.create_model(\"resnet50\", pretrained=True, features_only=True, out_indices=(1, 2, 3, 4))\n",
    "\n",
    "# Transfer Weights and Buffers\n",
    "for name_backbone, parameter_backbone in timm_backbone.named_parameters():\n",
    "    for name, parameter in model.model.backbone.conv_encoder.model.named_parameters():\n",
    "        if name_backbone == name:\n",
    "            parameter.data.copy_(parameter_backbone.data)\n",
    "\n",
    "# Transfer buffers (e.g., running mean and variance in BatchNorm)\n",
    "for name_backbone, buffer_backbone in timm_backbone.named_buffers():\n",
    "    for name, buffer in model.model.backbone.conv_encoder.model.named_buffers():\n",
    "        if name_backbone == name:\n",
    "            buffer.data.copy_(buffer_backbone.data)\n",
    "\n",
    "print((model.model.backbone.conv_encoder.model.conv1.weight == timm_backbone.conv1.weight).all())\n",
    "\n",
    "print(\"Number of classes:\", model.class_labels_classifier.out_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is relatively small, freezing the ResNet backbone will prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the backbone of the DETR model\n",
    "for param in model.model.backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the architechture of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetrForObjectDetection(\n",
      "  (model): DetrModel(\n",
      "    (backbone): DetrConvModel(\n",
      "      (conv_encoder): DetrConvEncoder(\n",
      "        (model): FeatureListNet(\n",
      "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "          (bn1): DetrFrozenBatchNorm2d()\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "          (layer1): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): DetrFrozenBatchNorm2d()\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer2): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): DetrFrozenBatchNorm2d()\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (3): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer3): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): DetrFrozenBatchNorm2d()\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (3): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (4): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (5): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer4): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): DetrFrozenBatchNorm2d()\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): DetrFrozenBatchNorm2d()\n",
      "              (act1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): DetrFrozenBatchNorm2d()\n",
      "              (drop_block): Identity()\n",
      "              (act2): ReLU(inplace=True)\n",
      "              (aa): Identity()\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): DetrFrozenBatchNorm2d()\n",
      "              (act3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (position_embedding): DetrSinePositionEmbedding()\n",
      "    )\n",
      "    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (query_position_embeddings): Embedding(100, 256)\n",
      "    (encoder): DetrEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x DetrEncoderLayer(\n",
      "          (self_attn): DetrAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): ReLU()\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): DetrDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x DetrDecoderLayer(\n",
      "          (self_attn): DetrAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): DetrAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (class_labels_classifier): Linear(in_features=256, out_features=13, bias=True)\n",
      "  (bbox_predictor): DetrMLPPredictionHead(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up Optimizer and Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.1)  # Updated step_size for 100 epochs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3.6737, Time: 1362.32 seconds\n",
      "Skipping batch 1153: No valid targets.\n",
      "Skipping batch 4476: No valid targets.\n",
      "Skipping batch 6255: No valid targets.\n",
      "Epoch 2/50, Loss: 3.0280, Time: 1309.49 seconds\n",
      "Skipping batch 1666: No valid targets.\n",
      "Epoch 3/50, Loss: 2.9384, Time: 1314.49 seconds\n",
      "Epoch 4/50, Loss: 2.8998, Time: 1335.51 seconds\n",
      "Epoch 5/50, Loss: 2.8662, Time: 1298.14 seconds\n",
      "Skipping batch 1577: No valid targets.\n",
      "Skipping batch 2594: No valid targets.\n",
      "Skipping batch 4338: No valid targets.\n",
      "Epoch 6/50, Loss: 2.8346, Time: 1332.59 seconds\n",
      "Skipping batch 2248: No valid targets.\n",
      "Skipping batch 4461: No valid targets.\n",
      "Epoch 7/50, Loss: 2.8127, Time: 1333.54 seconds\n",
      "Skipping batch 282: No valid targets.\n",
      "Skipping batch 733: No valid targets.\n",
      "Skipping batch 6021: No valid targets.\n",
      "Epoch 8/50, Loss: 2.7891, Time: 1286.97 seconds\n",
      "Epoch 9/50, Loss: 2.7606, Time: 1287.29 seconds\n",
      "Skipping batch 3982: No valid targets.\n",
      "Skipping batch 5660: No valid targets.\n",
      "Epoch 10/50, Loss: 2.7385, Time: 1337.20 seconds\n",
      "Skipping batch 1601: No valid targets.\n",
      "Epoch 11/50, Loss: 2.7201, Time: 1292.64 seconds\n",
      "Skipping batch 1034: No valid targets.\n",
      "Skipping batch 4390: No valid targets.\n",
      "Skipping batch 6137: No valid targets.\n",
      "Epoch 12/50, Loss: 2.7084, Time: 1300.57 seconds\n",
      "Skipping batch 4366: No valid targets.\n",
      "Skipping batch 5725: No valid targets.\n",
      "Epoch 13/50, Loss: 2.6799, Time: 1328.53 seconds\n",
      "Epoch 14/50, Loss: 2.6575, Time: 1323.30 seconds\n",
      "Skipping batch 567: No valid targets.\n",
      "Skipping batch 3319: No valid targets.\n",
      "Skipping batch 3693: No valid targets.\n",
      "Epoch 15/50, Loss: 2.6401, Time: 1285.75 seconds\n",
      "Skipping batch 2238: No valid targets.\n",
      "Skipping batch 2989: No valid targets.\n",
      "Skipping batch 4398: No valid targets.\n",
      "Epoch 16/50, Loss: 2.6193, Time: 1306.55 seconds\n",
      "Skipping batch 3020: No valid targets.\n",
      "Epoch 17/50, Loss: 2.5983, Time: 1312.92 seconds\n",
      "Epoch 18/50, Loss: 2.6004, Time: 1309.05 seconds\n",
      "Skipping batch 289: No valid targets.\n",
      "Skipping batch 5175: No valid targets.\n",
      "Epoch 19/50, Loss: 2.5773, Time: 1328.82 seconds\n",
      "Epoch 20/50, Loss: 2.5493, Time: 1307.00 seconds\n",
      "Epoch 21/50, Loss: 2.4962, Time: 1290.62 seconds\n",
      "Skipping batch 893: No valid targets.\n",
      "Skipping batch 1602: No valid targets.\n",
      "Epoch 22/50, Loss: 2.4812, Time: 1291.01 seconds\n",
      "Skipping batch 6231: No valid targets.\n",
      "Epoch 23/50, Loss: 2.4802, Time: 1294.06 seconds\n",
      "Skipping batch 3348: No valid targets.\n",
      "Skipping batch 6042: No valid targets.\n",
      "Epoch 24/50, Loss: 2.4646, Time: 1307.06 seconds\n",
      "Skipping batch 726: No valid targets.\n",
      "Skipping batch 3489: No valid targets.\n",
      "Skipping batch 4049: No valid targets.\n",
      "Epoch 25/50, Loss: 2.4624, Time: 1303.72 seconds\n",
      "Epoch 26/50, Loss: 2.4679, Time: 1270.48 seconds\n",
      "Epoch 27/50, Loss: 2.4584, Time: 1329.60 seconds\n",
      "Skipping batch 660: No valid targets.\n",
      "Epoch 28/50, Loss: 2.4572, Time: 1273.35 seconds\n",
      "Skipping batch 446: No valid targets.\n",
      "Skipping batch 917: No valid targets.\n",
      "Skipping batch 3110: No valid targets.\n",
      "Epoch 29/50, Loss: 2.4549, Time: 1309.71 seconds\n",
      "Skipping batch 2514: No valid targets.\n",
      "Epoch 30/50, Loss: 2.4473, Time: 1287.78 seconds\n",
      "Skipping batch 2789: No valid targets.\n",
      "Skipping batch 3814: No valid targets.\n",
      "Skipping batch 4389: No valid targets.\n",
      "Epoch 31/50, Loss: 2.4464, Time: 1296.07 seconds\n",
      "Skipping batch 595: No valid targets.\n",
      "Skipping batch 1660: No valid targets.\n",
      "Epoch 32/50, Loss: 2.4354, Time: 1295.75 seconds\n",
      "Epoch 33/50, Loss: 2.4446, Time: 1283.51 seconds\n",
      "Epoch 34/50, Loss: 2.4386, Time: 1316.23 seconds\n",
      "Skipping batch 770: No valid targets.\n",
      "Skipping batch 2001: No valid targets.\n",
      "Skipping batch 3706: No valid targets.\n",
      "Epoch 35/50, Loss: 2.4352, Time: 1298.83 seconds\n",
      "Skipping batch 287: No valid targets.\n",
      "Skipping batch 2985: No valid targets.\n",
      "Epoch 36/50, Loss: 2.4271, Time: 1342.06 seconds\n",
      "Epoch 37/50, Loss: 2.4270, Time: 1324.65 seconds\n",
      "Skipping batch 3480: No valid targets.\n",
      "Skipping batch 5495: No valid targets.\n",
      "Epoch 38/50, Loss: 2.4245, Time: 1371.62 seconds\n",
      "Epoch 39/50, Loss: 2.4170, Time: 1344.24 seconds\n",
      "Epoch 40/50, Loss: 2.4180, Time: 1340.43 seconds\n",
      "Skipping batch 5600: No valid targets.\n",
      "Epoch 41/50, Loss: 2.4124, Time: 1351.05 seconds\n",
      "Skipping batch 3292: No valid targets.\n",
      "Epoch 42/50, Loss: 2.4045, Time: 1327.67 seconds\n",
      "Epoch 43/50, Loss: 2.4087, Time: 1343.14 seconds\n",
      "Skipping batch 688: No valid targets.\n",
      "Epoch 44/50, Loss: 2.4129, Time: 1300.52 seconds\n",
      "Skipping batch 247: No valid targets.\n",
      "Epoch 45/50, Loss: 2.4122, Time: 1299.62 seconds\n",
      "Skipping batch 2999: No valid targets.\n",
      "Epoch 46/50, Loss: 2.4086, Time: 1308.77 seconds\n",
      "Skipping batch 1393: No valid targets.\n",
      "Epoch 47/50, Loss: 2.4029, Time: 1397.85 seconds\n",
      "Skipping batch 4327: No valid targets.\n",
      "Epoch 48/50, Loss: 2.4025, Time: 1394.69 seconds\n",
      "Epoch 49/50, Loss: 2.4114, Time: 1389.09 seconds\n",
      "Skipping batch 4482: No valid targets.\n",
      "Skipping batch 5736: No valid targets.\n",
      "Epoch 50/50, Loss: 2.3946, Time: 1379.88 seconds\n",
      "Total Training Time: 65955.80 seconds\n"
     ]
    }
   ],
   "source": [
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop for 100 epochs\n",
    "num_epochs = 50  # Updated from 10 to 100\n",
    "\n",
    "# Record the total training start time\n",
    "total_training_start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Record the start time for the epoch\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for step, (pixel_values, targets) in enumerate(train_loader):\n",
    "        # Move pixel values to device\n",
    "        pixel_values = torch.stack(pixel_values).to(device)\n",
    "\n",
    "        # Filter valid targets\n",
    "        valid_targets = []\n",
    "        for t in targets:\n",
    "            if \"class_labels\" in t and len(t[\"class_labels\"]) > 0:\n",
    "                valid_targets.append({\n",
    "                    \"class_labels\": t[\"class_labels\"].to(device),\n",
    "                    \"boxes\": t[\"boxes\"].to(device)\n",
    "                })\n",
    "\n",
    "        # Add properly formatted dummy targets if necessary\n",
    "        while len(valid_targets) < len(pixel_values):\n",
    "            valid_targets.append({\n",
    "                \"class_labels\": torch.empty((0,), dtype=torch.int64).to(device),\n",
    "                \"boxes\": torch.empty((0, 4), dtype=torch.float32).to(device)\n",
    "            })\n",
    "\n",
    "        # Skip the batch if no valid targets exist\n",
    "        if all(len(t[\"class_labels\"]) == 0 for t in valid_targets):\n",
    "            print(f\"Skipping batch {step}: No valid targets.\")\n",
    "            continue\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=pixel_values, labels=valid_targets)\n",
    "        loss = outputs.loss  # Loss is computed internally by DETR\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Record the end time for the epoch and calculate the duration\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "\n",
    "    # Print epoch summary with time taken\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}, Time: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "# Record the total training end time and calculate the duration\n",
    "total_training_end_time = time.time()\n",
    "total_training_duration = total_training_end_time - total_training_start_time\n",
    "\n",
    "# Print total training time\n",
    "print(f\"Total Training Time: {total_training_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DETR/detr-processor/preprocessor_config.json']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"DETR/detr-finetuned\")\n",
    "processor.save_pretrained(\"DETR/detr-processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainability Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grad-CAM for DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6123423,
     "sourceId": 9956307,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
